[{"content":"Introduction In my last post, I created a Cisco SD-WAN assistant to help me run NWPI traces and troubleshoot the network. The interaction with the assistant required the user to answer questions until receiving information about a particular flow and potential problems. In this post my goal is to use multiple agents and see if I can get to the same conclusion with less human interaction. Repository can be found here\nGetting Started To achieve this, I will use LangGraph officially defined as:\nA library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows\nThere are different approaches, but I decided to build a structure where there is a supervisor that orchestrates the workflow and decides who should act next. The idea is to build a graph that represents the agents and how they are connected. The graph illustrates the order in which agents can be executed.\nIn my case I have 3 agents:\nSupervisor - This agent is in charge of receiving user input and deciding who should act next. Also, once others agents finish their tasks, they will report back to it and a new routing decision will be made. The supervisor is the only agent that can decide when to go back to the user with a response. Reviewer - This agent will review the information that will be sent back to the user, perform some summation and resolve questions or situations the tracer might raise. Tracer - This is the agent that will run the traces and retrieve the information that the user is looking for. It will report back to the supervisor when done or if any questions need to be answered. I had to modify the prompt of the tracer a little bit, so I could get a behavior that is better for this approach. Also, each agent can have its own tools. Currently, the tracer has more tools than the rest of agents.\nVisualized, the graph looks like this:\nThe dotted arrow indicate a conditional edge, meaning that the supervisor can decide what the next agent should be or if ending is appropriate.\nThe continuous arrow indicate the next step that must be taken. For example, after \u0026ldquo;start\u0026rdquo; the next agent must be the supervisor. Tracer and Reviewer must go to the supervisor.\nEven though this is a simple graph, this supervisor approach is very powerful.\nHow the supervisor routes? The supervisor plays a critical role as it determines who should act next. This is defined in the following function:\noptions = [\u0026#34;FINISH\u0026#34;] + members function_def = { \u0026#34;name\u0026#34;: \u0026#34;route\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Select the next role.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;routeSchema\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;next\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Next\u0026#34;, \u0026#34;anyOf\u0026#34;: [ {\u0026#34;enum\u0026#34;: options}, ], } }, \u0026#34;required\u0026#34;: [\u0026#34;next\u0026#34;], }, } With this, every time the supervisor receives any input, it will come up with a \u0026ldquo;next\u0026rdquo; value from the available options and this will represent the next node on the graph.\nDemo We will use the following topology to test\nI tell the assistant information about the issue and inform that there is traffic passing already (traffic is required in order for NWPI to generate the insights we are looking for).\nThis is what the Assistant came back with\nThe response presents information in a condensed way, indicating the hops and path the traffic is taking and some events detected for this communication. The details of one of the flows is also present, however is has less information than before. This is because I have asked the reviewer agent to keep what it considers more relevant and send it to the user. In the end, we can see that the DROP_REPORT is mentioned and there is a suggestion to review ACLs ðŸŽ‰\nWe can play with the reviewer prompt to display more information about the details of the flow.\nBehind the scenes Ok, the assistant came back with a good response, but let\u0026rsquo;s see in more detail what happened under the hood.\nUsing LangSmith we can get details and insights about the workflow followed. Here is the complete process.\nFirst, the supervisor receives the user\u0026rsquo;s query and pass it to the tracer.\nNext, the tracer uses the available tools to starts the trace, waits to capture some flows and retrieves information. Reports to the supervisor. Note that the order in which tools are executed is up to the agent.\nThen, the supervisor receives the information and decides the reviewer should act next.\nNext, the reviewer receives the information and rewrites what was received from the tracer. Goes back to the supervisor.\nThe supervisor decides the information is ready to be sent to the user. This is when we receive the message back on Webex.\nLessons Learned Since agents can make decisions, it is not always easy to understand what they are doing or why they return what they return, using LangSmith definitely helped with this. Not only we can see the order and tools used, but also there is some metadata that provides additional valuable information. I got to some situations where the supervisor was calling the tracer multiple times due to some error retrieving information. In the end, this was caused by error on the code and, fortunately, my assistant isn\u0026rsquo;t expensive. However, if your use case consumes a lot of tokens you should consider add some sort of safeguard to prevent a loop that grows your API consumption. Talking about cost, small language models are a good alternative. After running out of quota, I remembered gpt-4o-mini model is out and decided to give it a try. After some testing I saw it performed very well and was much cheaper, so I stuck to it. Conclusion Using multi-agent deployment we can achieve more complex tasks and have more flexibility. If needed, user interaction can be added on certain decisions that are important. Also, there is some added complexity as the prompts have to be refined to achieve the results we expect. In my case I had to do multiple iterations and refinements to all agents\u0026rsquo; prompts before getting an output that I considered to be good enough. I am interested in testing other approaches to multi-agent deployments and adding additional info for agents to provide more accurate information through RAG.\n","permalink":"http://localhost:1313/improving-my-sd-wan-assistant-multiple-agents/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn my \u003ca href=\"/building-my-first-sd-wan-ai-assistant-with-langchain/\"\u003elast post\u003c/a\u003e, I created a Cisco SD-WAN assistant to help me run NWPI traces and troubleshoot the network. The interaction with the assistant required the user to answer questions until receiving information about a particular flow and potential problems. In this post my goal is to use multiple agents and see if I can get to the same conclusion with less human interaction. Repository can be found \u003ca href=\"https://github.com/aruiz-p/sdwan-langgraph\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"getting-started\"\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eTo achieve this, I will use \u003ca href=\"https://langchain-ai.github.io/langgraph/\"\u003eLangGraph\u003c/a\u003e officially defined as:\u003c/p\u003e","title":"Improving my SD-WAN Assistant - Multiple agents"},{"content":"Introduction It has been a while since I wanted to hop on to the LLM train and learn how to use one of the popular frameworks. A few months back, I saw a great Cisco Live presentation by my good friend Jesus, and it gave me the determination I needed to finally dive deeper into the topic.\nSince then, I have been doing research and thinking about a nice use case for me to put as objective of my learning process. After considering different options, I decided to build an SD-WAN AI assistant that could help me troubleshoot an SD-WAN issue. Taking advantage of the available tools, I decided that my assistant would be an expert on the Net work Wide Path Insights functionality.\nIn this post, I want to share a bit of my experience building it and of course show some of the results. For a better understanding I suggest to have the G ithub repo opened as you go through the post.\nAbout the setup My SD-WAN Lab is running 20.12.3 on the Manager and the Wan Edges are using 17.9.4a. I have a very simple topology that looks something like this:\nThe programming language used is Python and the framework I chose to interact with the LLM is LangChain. I used OpenAI model gpt-4o and a Webex bot for the interaction. The repo can be found here.\nMy objective For the sake of context, troubleshooting inside a SD-WAN fabric is not easy because traffic is encrypted, policies dictate how traffic will flow, there could be multiple paths to a destination, next hops can be changed with policies, there are multiple hops involved, and more. Figuring out all this information is time consuming and not a straightforward process.\nNWPI Trace is a tool that greatly improves the troubleshooting process as it will give hop-by-hop information and visibility. It can be easily started from the Manager\u0026rsquo;s UI, it will detect flows based on specified filters and you can browse around to get all the visibility you need. It is a very complex and complete tool. As I described before, I wanted to use this project as a playground to learn and since I didn\u0026rsquo;t have prior experience with LLMs or LangChain I set a simple objective:\nBuild an assistant that can start a NWPI trace and give me details of the flows.\nPlanning and Building Ok, I had my objective, but how to start?\nI took a hands-on approach which meant that I didn\u0026rsquo;t learn LangChain from scratch and instead took the Cisco Live session repo as a base and built on top of that. The reasons to choose this repo were simple:\nIt was explained on the sessions so I had a general idea of the technologies and its purpose. I thought it would be easy to adjust to my use case (Eg. I also use Webex, I will be interacting with network devices, I saw how the tools could be replaced with my own) I had to do some cleaning before starting, this required me to understand what was essential to host the LLM and interact with it. Luckily, the repo had an organized structure that made it easy to understand.\nFrom the session, I learned about LangChain Tools, so I knew I could create functions that my agent could use to perform different actions. In this case, actions would be something like starting those traces and getting info from them\nChallenge 1 I needed to get familiar with the NWPI API, at this point I knew I had seen somewhere on the API documentation that some operations were available, but never had taken the time to analyze them. To my surprise, the specific actions of starting a trace and getting details of it, were not included\u0026hellip; There was information about starting a \u0026ldquo;task\u0026rdquo; a.k.a \u0026ldquo;Auto-on Task\u0026rdquo;, which is not the same as the \u0026ldquo;Trace\u0026rdquo; I had in mind. At this point, I needed to decide if I would go for the \u0026ldquo;official\u0026rdquo; and maybe easier way or exploring an alternative to achieve exactly what I wanted.\nKnowing that almost everything is API driven, I used the inspect tab of my browser and started exploring the APIs triggered when I started a trace through the UI. After a first quick pass, I determined it was doable and started gathering the information I needed.\nChallenge 2 I already knew I would have to do some analysis to make my idea a reality, but I underestimated how much I would need to do. In fact, the difficulty of this task kept me away from the project for some time as it became increasingly complex.\nIn my mind there were only 3 \u0026ldquo;simple\u0026rdquo; tasks:\nFind the API to start the trace Find the API to confirm the trace is running Find the API to give me details of the flows Find the API to start the trace Starting a trace from the UI is very simple, you just need a Site ID and a VPN ID. However, there are underlying verifications happening that we take for granted.\nThe site ID is really needed to identify the devices to start the trace on. There are a bunch of options (QoS insights, ART visibility, APP visibility, DIA, etc) that are version dependent. The VPN needs to exist. To get this done I created the function get_device_details_from_site so I could find related information of the devices to start the trace. I needed:\nversions serial numbers names reachability status. Then, I created the start_trace function that would receive the information previously obtained and other filters. I kept the filters as simple as possible, leaving only an option to specify a source and destination subnet. There are a lot of trace options for which I didn\u0026rsquo;t do any type of version verification before running it, I just did it for the QoS insights that requires version 17.9 or later. This function returns some information needed later to verify the status.\nFind the API to confirm the trace is running This was probably the easiest task. I created the verify_trace_state function and with the help of the LLM it can be ran some seconds after starting the trace. It returns the state, which is also needed to get information later on.\nFind the API to give me details of the flows This was the most complex and time consuming task. In my mind checking the result of a trace is very simple, however when we receive the information in chunks, through different calls it starts getting tricky.\nI tried to replicate the process I go through on the UI:\nView the insights of the trace and check the flows that were captured (if any) For the list of flows, look for the one that had the \u0026ldquo;readout\u0026rdquo; button in red (problem detected) and click to get more details. Expand the flow view to get access to the advanced functionalities so I can determine the features that the packet is going through on each of the hops. To get the flows captured for the trace I created the function get_flow_summary. This function will return the list of captured flows, You will see details like src/dst, application and protocol. This is useful to identify the flow id that you are interested in getting more details.\nI created the trace_readout function to get a summary of the events that the trace captured along with the affected path. For example, you could see that an SSH flow is not working between Device X and Device Y.\nOk, once you have identified the flow and events you are interested in, you can get detailed information of the flow with the function get_flow_detail. This will give you hop by hop information like:\nHop Event Local/Remote colors Ingress/Egress interfaces Ingress/Egress features applied to packets Feature making the forward decision With this information is possible to see all sort of things, like ACLs, type of policies applied, why a packet was routed through a specific color, drops, confirm your policy is working as expected, etc.\nOk, I think that\u0026rsquo;s it!\nDemo I started by creating an ACL to block communication and applied it on the DC side.\nMunich_DC100-1 - ACL configuration sdwan interface GigabitEthernet2 access-list ACL_Drop_172_16_10_0 out policy access-list ACL_Drop_172_16_10_0 sequence 1 match source-ip 172.16.10.0/24 destination-ip 172.16.100.0/24 ! action drop count dropCounter ! ! default-action accept ! Will my assistant detect this? ðŸ¤”\nNext, I start the application and request the LLM to start a trace. I can confirm on the UI that it is created.\nI start a couple of SSH connections from branch to DC\nThen, I ask the assistant if flows have been captured, it responds with this\nWe can see flows were captured and also it gave me more information of the events detected and the path with device names. The first event seem to be related to our issue. So far the information looks accurate, let\u0026rsquo;s get more details.\nWith this, we can see that the client sent multiple ssh attempts, we can dig further into one of the flows. Let\u0026rsquo;s see what else it gives.\nFinally, the assistant provides detailed information about the features that each of the hops apply to the traffic. On the second hop in Munich DC we can see that egress features show the SD-WAN ACL and a Drop Report. The assistant provides its own conclusion and it is also suspecting that Munich router is dropping the traffic. With a little bit more work, the agent could be able to tell the name of the ACL and sequence number that is dropping the traffic. We have successfully identified the root of the issue!! ðŸ˜€ ðŸŽ‰\nLessons learned When I started, I wanted to be super cautious with the credits ($$) so I was using gpt-3.5-turbo-16k that is cheaper but also less intelligent. At some point, I faced issues with the LLM getting into a loop of problems, I decided to test out gpt-4o and felt a difference on the way the agent was reasoning. Initially, I was using an LLM temperature = 0, this was ok, but the responses were lacking variety and details, I needed to make it more chatty. Tweaking the temperature = 0.9 gave me a good balance between chattiness and correctness (although sometimes the agent still gives information that is questionable based on the outputs) Troubleshooting problems could be difficult at times, mostly I relied on printing outputs while the functions were executed and the agent outputs on the terminal. It let me understand what tools the agent was using and the order. Also, I could see what the tools were returning. Here is an example: The text in green indicate the tools the agent is accessing. The yellow text is the information returned by a function. In this case, we can see that the agent called \u0026quot;get_entry_time_and_state\u0026quot; function so it can get information needed to call the next function \u0026quot;get_flow_detail\u0026quot;\nThere are better tools available to help with troubleshooting like LangSmith Tr acing, I will explore it for future use.\nThe system prompt of my agent had to be refined several times, I often found that I needed to provide more details to handle certain situations correctly, especially when the output of a function was needed to call another one or to handle unexpected situations. I think it can still be improved, in fact I want to write a totally different prompt to try and make the agent run all of the tools by itself and just return a conclusion after analyzing all the outputs. Conclusion All in all, it was a good (and long) exercise to learn and build my first assistant. I feel happy with the result as I was able to get to me objective. At the same time, I recognize there are a lot of things that could be improved to make the results more reliable and meaningful. Also, there is much more information that NWPI can show, so the tools can definitely be extended.\nAs a next step, I am planning to learn LangChain properly and understand how can I implement multiple agents to enhance the functionality and reliability of my assistant.\nI hope this post will help you in the same way that Cisco Live presentation helped me!\n","permalink":"http://localhost:1313/building-my-first-sd-wan-ai-assistant-with-langchain/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIt has been a while since I wanted to hop on to the LLM train and learn how to use one of the popular frameworks. A few months back, I saw a great \u003ca href=\"https://www.ciscolive.com/on-demand/on-demand-library.html?search=jesus\u0026amp;search=jesus#/session/1707505627331001pilj\"\u003eCisco Live presentation\u003c/a\u003e by my good friend Jesus, and it gave me the determination I needed to finally dive deeper into the topic.\u003c/p\u003e\n\u003cp\u003eSince then, I have been doing research and thinking about a nice use case for me to put as objective of my learning process. After considering different options, I decided to build an SD-WAN AI assistant that could help me troubleshoot an SD-WAN issue. Taking advantage of the available tools, I decided that my assistant would be an expert on the \u003ca href=\"/network-wide-path-insights-an-introduction/\"\u003eNet\u003c/a\u003e \u003ca href=\"/network-wide-path-insights-an-introduction/\"\u003ework Wide Path Insights\u003c/a\u003e functionality.\u003c/p\u003e","title":"Building my first SD-WAN AI Assistant with LangChain"},{"content":"Introduction This post is part of DEVNET DevOps (300-910) â€“ Blueprint\nAnsible is an open-source automation tool used for configuration management, application deployment and task automation. Although it can be used for a lot of purposes, we are particularly interested on network automation related tasks.\nThere are a lot of resources to learn Ansible online, these notes should serve only as guidance, but you are encouraged to go deeper at your own pace.\nKey Features Agentless: Ansible doesn\u0026rsquo;t require agents to be installed on managed nodes, making it lightweight and easy to deploy. YAML-based: Ansible playbooks and configuration files are written in YAML, human-readable and easy to understand. Idempotent: Ansible ensures that the desired state of the system is maintained, making it safe to run multiple times. Components Control Node: The machine where Ansible is installed and from which automation is initiated. Managed Nodes: The machines managed by Ansible. Ansible connects to managed nodes via SSH (Linux) or WinRM (Windows) to execute tasks. Inventory: A list of managed nodes that Ansible will manage. Playbooks: YAML files containing a set of tasks to be executed on managed nodes. Modules: Units of code that Ansible executes on managed nodes to perform tasks. Requirements to Use Ansible Control Node requirements Ansible requires python to be installed Can be installed with yum, apt or pip Most commonly installed on Linux or macOS Inventory File Text file containing a list of managed nodes that Ansible can connect to and manage in this case our network devices. It can be in INI or YAML format You can group nodes together and define variables. See multiple examples here Playbook YAML files that define a set of tasks to be executed on managed nodes. They are the heart of Ansible automation.\nStructure Hosts: Specifies the target hosts or groups (inventory) Tasks: Contains a list of tasks to be executed on managed nodes. Variables: Defines variables used within the playbook. Playbooks use modules to interact with the managed modes . I recommend getting familiar with some Cisco modules\nPractical Examples You can find some practical examples on the following GitHub repository.\nBack to BluePrint ","permalink":"http://localhost:1313/ansible-notes/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"/study-devops-blueprint/\"\u003e\u003cem\u003eThis post is part of DEVNET DevOps (300-910) â€“ Blueprint\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAnsible is an open-source automation tool used for configuration management, application deployment and task automation. Although it can be used for a lot of purposes, we are particularly interested on network automation related tasks.\u003c/p\u003e\n\u003cp\u003eThere are a lot of resources to learn Ansible online, these notes should serve only as guidance, but you are encouraged to go deeper at your own pace.\u003c/p\u003e","title":"Ansible Notes"},{"content":"Introduction Docker helps with packaging, distributing, and running applications. However, we should be responsible so we can ensure the security of the Docker environment. There are some best practices and strategies we can follow to achieve this objective.\nSecurity best practices and strategies No sensitive information on files Dockerfiles are used to define the steps needed to build Docker images. It\u0026rsquo;s crucial to avoid including sensitive information such as credentials, API keys, or other secrets directly in Dockerfiles. Keep in mind these files are often version-controlled and may be accessible to unauthorized users, resulting in a significant security risk.\nConsider using environment variables or external configuration files to inject sensitive information into your Docker containers at runtime.\nImplement Role-Based Access Control (RBAC) Role-Based Access Control (RBAC) is a security principle that restricts system access based on the roles and privileges assigned to users or groups. In the context of Docker, RBAC can limit the capabilities of containers and prevent unauthorized access to sensitive resources.\nConsider implementing RBAC mechanisms such as:\nUsing Docker\u0026rsquo;s native Role-Based Access Control (RBAC) features to define granular permissions for users and services. Restricting container capabilities using Docker Security Profiles (Seccomp, AppArmor, or SELinux) to minimize the potential impact of security breaches. Regularly reviewing and updating access controls to ensure they align with the principle of least privilege and reflect changes in your environment. Use environment variables and files Environment variables are mechanism for passing configuration information to Docker containers. They allow us to decouple application configuration from the container image, so we can customize behavior across different environments without modifying the underlying image.\nWhen using environment variables, we need to follow security best practices:\nAvoid hardcoding sensitive information directly into Dockerfiles or source code. Use encrypted secrets management tools to securely store and retrieve sensitive data. Limit access to environment variables containing sensitive information to authorized users and services. We can use Docker\u0026rsquo;s built-in support for environment files ( --env-file) to load environment variables from external files.\n### Content of my_env_vars.env ### DB_HOST=localhost DB_USER=admin DB_PASSWORD=secretpassword Dockerfile consuming the file\n# Dockerfile FROM ubuntu:latest COPY . /app # Load environment variables from file ENV_FILE=my_env_vars.env RUN set -o allexport; source $ENV_FILE; set +o allexport CMD [\u0026#34;./app\u0026#34;] To run build and run\n# Build the Docker image docker build -t myapp . # Run the Docker container with environment variables from file docker run --env-file my_env_vars.env myapp Go back to Blueprint ","permalink":"http://localhost:1313/docker-safety-practices/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eDocker helps with packaging, distributing, and running applications. However, we should be responsible so we can ensure the security of the Docker environment. There are some best practices and strategies we can follow to achieve this objective.\u003c/p\u003e\n\u003ch2 id=\"security-best-practices-and-strategies\"\u003eSecurity best practices and strategies\u003c/h2\u003e\n\u003ch3 id=\"no-sensitive-information-on-files\"\u003eNo sensitive information on files\u003c/h3\u003e\n\u003cp\u003eDockerfiles are used to define the steps needed to build Docker images. It\u0026rsquo;s crucial to avoid including sensitive information such as credentials, API keys, or other secrets directly in Dockerfiles. Keep in mind these files are often version-controlled and may be accessible to unauthorized users, resulting in a significant security risk.\u003c/p\u003e","title":"Docker - Safety practices"},{"content":"Introduction In the CI/CD world, Deploying updates efficiently and with minimal disruption is crucial. There are different release deployment strategies that organization can adopt including Big-Bang, Rolling, Blue-Green, and Canary.\nDeployment Option Big Bang Deployment All changes are bundled together and deployed at once - this is the essence of Big Bang Deployment. While it simplifies the deployment process, as everything is pushed out simultaneously, it also poses significant risks. If any issues arise during deployment, they affect the entire system immediately, potentially leading to widespread downtime or malfunctions. Best suited for smaller projects or those with low complexity where the impact of failures is minimal.\nDowntime required, all or nothing approach All the systems are the same, no variation from one another. No production traffic can be tested until the app is in production. (limited testing) Rolling Deployment Takes a more cautious approach by gradually rolling out updates across different segments of the infrastructure. This strategy involves updating a subset of servers or components at a time while keeping the rest of the system operational. Incrementally deploying changes reduces the risk of system-wide failures and allows for quicker rollback if issues arise. It\u0026rsquo;s particularly effective for large-scale applications with distributed architectures, as it ensures continuous availability while updates are being applied.\nUpgrades individual components at a time Blue-Green Deployment Two identical production environments, referred to as Blue and Green, are maintained simultaneously. While one environment (let\u0026rsquo;s say Blue) serves live traffic, the other (Green) remains idle. When updates are ready for deployment, they are applied to the inactive environment (Green). Once the updates are successfully deployed and tested, traffic is switched from the Blue to the Green environment, effectively swapping their roles. This approach offers zero-downtime deployments and provides a straightforward rollback mechanism by reverting to the previous environment if issues arise.\nIt\u0026rsquo;s very expensive to have two identical deployments Canary Idea is to release updates to a small subset of users (canary group) or servers before rolling them out to the entire system. By monitoring the behavior and performance of the canary group, developers can assess the impact of the changes and detect any potential issues early on. If the canary group performs as expected, the updates are gradually expanded to the wider audience. If issues arise, the deployment can be halted or rolled back before affecting the entire system. Canary Deployment is ideal for projects where thorough testing and risk mitigation are crucial, allowing for controlled experimentation without affecting the entire user base.\nComparison Table Deployment StrategyAdvantagesConsiderationsZero down-timeComplexity****Big BangSimplifies deployment process\n- Requires minimal coordinationHigh risk of system-wide failures\n- Limited rollback optionsNo1RollingReduces risk of widespread failures\n- Allows for quicker rollback\n- Ensures continuous availability during deploymentRequires careful monitoring and coordination to ensure smooth transition\n- May prolong deployment process for large-scale applicationsYes2Blue-Green- Provides straightforward rollback mechanismRequires additional infrastructure resources to maintain parallel environments\n- Initial setup and configuration can be complex\n- Very expensive to maintain Yes3CanaryAllows for controlled experimentation and testing\n- Early detection of potential issues\n- Minimizes risk by limiting impact to a small subset of users or serversRequires careful selection and monitoring of the canary group\n- May not be suitable for all applications, especially those with small user bases or where changes impact the entire system equallyYes3\nGo back to Blueprint ","permalink":"http://localhost:1313/ci-cd-deployment-strategies/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the CI/CD world, Deploying updates efficiently and with minimal disruption is crucial. There are different release deployment strategies that organization can adopt including \u003cstrong\u003e\u003cem\u003eBig-Bang, Rolling, Blue-Green, and Canary.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"deployment-option\"\u003eDeployment Option\u003c/h2\u003e\n\u003ch3 id=\"big-bang-deployment\"\u003eBig Bang Deployment\u003c/h3\u003e\n\u003cp\u003eAll changes are bundled together and deployed at once - this is the essence of Big Bang Deployment. While it simplifies the deployment process, as everything is pushed out simultaneously, it also poses \u003cem\u003e\u003cstrong\u003esignificant risks\u003c/strong\u003e\u003c/em\u003e. If any issues arise during deployment, they affect the entire system immediately, potentially leading to widespread downtime or malfunctions. Best suited for smaller projects or those with low complexity where the impact of failures is minimal.\u003c/p\u003e","title":"CI/CD Deployment Strategies"},{"content":"Introduction The concept of golden image refers to an image that has been extensively tested and verified its fully operational. Following the same logic, an image that has not been tested or verified should not be considered a golden image. Organizations need to define the process to follow to mark an image as golden.\nThe process should include testing and verification methodologies. Also, from a security standpoint patches should be present and if there are external dependencies, it\u0026rsquo;s crucial to update them to their respective recommended versions.\nImage Tags Tagging Docker images plays a crucial role in managing and identifying different versions of images, including golden images.\nBenefits Identifying Versions - Tagging allows you to assign meaningful names to Docker images, making it easier to identify different versions or variations of an image. Version Control - By tagging golden images with version numbers or release names, you can maintain a history of changes and track which versions have been tested and approved. You can roll back if needed. Lifecycle Management - Tagging helps in managing the lifecycle of golden images by providing an indication of their status and purpose (\u0026ldquo;dev\u0026rdquo;, \u0026ldquo;qa\u0026rdquo;, \u0026ldquo;prod\u0026rdquo;) Promotion Pipeline - Tags can be used to signify the progression of images through different stages of testing and approval. Accessing Golden Images Images should be available and accessible through trusted registries. One example is the Docker Hub which is regularly monitored and updated based on vulnerabilities, fixes, etc. It is possible to have private registries and base images as well.\nCommon Locations Docker Hub - Cloud-based service provided by Docker that serves as a centralized repository for Docker images. You can have both public and private registries. Docker Trusted Registry (DTR) - Enterprise-grade image storage solution provided by Docker. Extends the capabilities of Docker Hub to provide enhanced security, compliance, and collaboration features for organizations with strict security and regulatory requirements. Private Docker registries - Private Docker registries are self-hosted repositories for storing Docker images within an organization\u0026rsquo;s infrastructure. Private Docker registries are typically deployed on-premises or within private cloud environments. Image lifecycle Typically, the steps to upload new images are the following:\nCreate our Dockerfile Build the image Tag the image Push the image Push Process These commands are used to upload Docker images to a registry:\ndocker login : This command authenticates the Docker client with the specified Docker registry URL. It prompts you to enter your username and password for authentication. Once authenticated, Docker stores an authentication token locally, allowing you to interact with the registry without needing to log in again until the token expires.\ndocker login registry.example.com docker build -t \u0026lt;image_name\u0026gt; -f . : This command builds a Docker image using the specified Dockerfile ( -f) and tags ( -t) it with the given image name. The . at the end of the command indicates that the build context is the current directory, where the Dockerfile is located. For example:\ndocker build -t datastore -f Dockerfile_datastore . docker push \u0026lt;registry_url\u0026gt;/\u0026lt;image_name\u0026gt;: : This command uploads the specified Docker image to the registry with the given URL. It includes the repository path ( \u0026lt;registry_url\u0026gt;/\u0026lt;image_name\u0026gt;:\u0026lt;tag\u0026gt;) where the image will be stored in the registry. Before pushing the image, ensure that you have tagged it correctly with the desired repository path. For example:\ndocker push registry.example.com/datastore:latest By following these commands, you can authenticate with the Docker registry, build your Docker image, and then push it to the registry for storage and distribution. This process allows to share the containerized applications with others or deploy them to production environments.\nAdditional References Docker Hub\nDocker Trusted Registry (DTR)\nPrivate Docker Registry\nDocker image build and push\nGo back to blueprint ","permalink":"http://localhost:1313/docker-golden-images/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe concept of golden image refers to an image that has been extensively tested and verified its fully operational. Following the same logic, an image that has not been tested or verified should not be considered a golden image. Organizations need to define the process to follow to mark an image as \u003cstrong\u003egolden\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe process should include testing and verification methodologies. Also, from a security standpoint patches should be present and if there are external dependencies, it\u0026rsquo;s crucial to update them to their respective recommended versions.\u003c/p\u003e","title":"Docker Golden Images"},{"content":"Introduction Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. This page describes the commands you can use in a Dockerfile.\nDocker runs instructions in a Dockerfile in order. A Dockerfile must begin with a FROM instruction.\nExample # Use the official Python image as the base image FROM python:3.9-slim # Set the working directory inside the container WORKDIR /app # Copy the Python script from the host machine into the container COPY my_script.py . # Run the Python script when the container starts CMD [\u0026#34;python\u0026#34;, \u0026#34;my_script.py\u0026#34;] FROM python:3.9-slim : Specifies that our image will be based on the official Python image with version 3.9, using the slim variant. WORKDIR /app : Sets the working directory inside the container to /app. Later instructions will be executed in this directory. COPY my_script.py . : Copies a file named my_script.py from the host machine into the /app directory of the container. The . refers to the current directory in the Docker context, which will typically be the directory containing the Dockerfile. CMD [\u0026quot;python\u0026quot;, \u0026quot;my_script.py\u0026quot;] : Specifies the default command to run when a container based on this image starts. In this case, it runs the Python interpreter ( python) with the my_script.py script as an argument. Table of Instructions FROM ENV COPY EXPOSE ADD WORKDIR RUN LABEL CMD\nInstructions in detail FROM\nSpecifies the base image to build the container. Every Dockerfile must start with a FROM instruction.\nFROM [--platform=\u0026lt;platform\u0026gt;] \u0026lt;image\u0026gt;[:\u0026lt;tag\u0026gt;] [AS \u0026lt;name\u0026gt;] ADD\nSpecifies the origin of a local file or URL and transfers the associated files to the designated destination directory. If the source is compressed, like a .tar file, the ADD instruction will automatically extract the contents after the copy operation is finished.\nADD [OPTIONS] \u0026lt;src\u0026gt; ... \u0026lt;dest\u0026gt; RUN\nExecutes shell commands within the image during the build process. This is typically used for tasks like package installation, environment setup, etc. Common commands include \u0026lsquo;apt install\u0026rsquo; and \u0026lsquo;pip install\u0026rsquo; for Python packages.\nRUN [OPTIONS] \u0026lt;command\u0026gt; ... WORKDIR\nSets the working directory for any subsequent RUN, CMD, ENTRYPOINT, COPY, and ADD instructions. It\u0026rsquo;s better to use absolute paths instead of relative ones (../)\nWORKDIR /path/to/workdir EXPOSE\nFunctions as a type of documentation instruction that you can use to indicate the ports that will be used for the container. Default protocol is TCP, but you can define UDP as well.\nEXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;/\u0026lt;protocol\u0026gt;...] CMD\nSpecifies the command to run when a container is started from the image. Can be overridden at runtime. You can only use it once.\nCMD command param1 param2 ENTRYPOINT\nSets a main command that will always be executed when the container starts up.\nENTRYPOINT command param1 param2 ENV\nSets environment variables in the image.\nENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... VOLUME\nCreates a mount point and marks it as externally mounted. Used for persisting data outside the container.\nVOLUME [\u0026#34;/data\u0026#34;] USER\nSets the username or UID to use when running the container.\nUSER \u0026lt;user\u0026gt;[:\u0026lt;group\u0026gt;] Additional references Official Dockerfile documentation - Check this page for further details of the commands.\nGo back to Blueprint ","permalink":"http://localhost:1313/dockerfile/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eDocker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. This page describes the commands you can use in a Dockerfile.\u003c/p\u003e\n\u003cp\u003eDocker runs instructions in a Dockerfile in order. A Dockerfile \u003cstrong\u003emust begin with a FROM instruction\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"example\"\u003eExample\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e# Use the official Python image as the base image\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eFROM python:3.9-slim\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e# Set the working directory inside the container\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eWORKDIR /app\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e# Copy the Python script from the host machine into the container\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eCOPY my_script.py .\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e# Run the Python script when the container starts\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eCMD [\u0026#34;python\u0026#34;, \u0026#34;my_script.py\u0026#34;]\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eFROM python:3.9-slim\u003c/code\u003e : Specifies that our image will be based on the official Python image with version 3.9, using the slim variant.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eWORKDIR /app\u003c/code\u003e : Sets the working directory inside the container to \u003ccode\u003e/app\u003c/code\u003e. Later instructions will be executed in this directory.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eCOPY my_script.py .\u003c/code\u003e : Copies a file named \u003ccode\u003emy_script.py\u003c/code\u003e from the host machine into the \u003ccode\u003e/app\u003c/code\u003e directory of the container. The \u003ccode\u003e.\u003c/code\u003e refers to the current directory in the Docker context, which will typically be the directory containing the Dockerfile.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eCMD [\u0026quot;python\u0026quot;, \u0026quot;my_script.py\u0026quot;]\u003c/code\u003e : Specifies the default command to run when a container based on this image starts. In this case, it runs the Python interpreter ( \u003ccode\u003epython\u003c/code\u003e) with the \u003ccode\u003emy_script.py\u003c/code\u003e script as an argument.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"table-of-instructions\"\u003e\u003cstrong\u003eTable of Instructions\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eFROM\nENV\nCOPY\nEXPOSE\nADD\nWORKDIR\nRUN\nLABEL\nCMD\u003c/p\u003e","title":"Dockerfile"},{"content":"Introduction As networks evolve to provide better user experience and new technologies to manage the network are introduced, maintaining everything running smoothly has become increasingly difficult. One of the critical responsibilities of the operations team is tracking problems happening all over the network. Identifying them is just the beginning, then they need to be logged and driven to resolution. Multiply the amount of actions per incident and you have enough to keep your IT team busy all day long!\nIn this post I will show you what you need to know to integrate the SD-WAN Manager with ServiceNow for incident management. We will see some of the most common problems in SD-WAN.\nLab setup I am using SD-WAN Manager version 20.12.1 and I have a ServiceNow developer instance. My Webhook Server runs in Ubuntu 20.04 LTS and I built the webhook receiver in Go language.\nTo simplify things, I have direct communication between all the elements of my lab.\nWebhooks Webhooks are a way for web applications to communicate with each other in real-time. They allow one application to send automated notifications to another application when a specific event occurs, it is referred to as push model. This facilitates integration between different systems and can be used to trigger subsequent automated activities. Webhooks typically use HTTP callbacks to share notifications/information.\nIn our scenario, the SD-WAN Manager will monitor events on BR10 and will send HTTP POST requests to our Webhook Server when specific events occur. This will allow us to manage incidents in ServiceNow.\nAnatomy of a Webhook Notification Let\u0026rsquo;s understand the structure and information that the SD-WAN Manager will be sharing with our Webhook Server.\nThis is an example of the information sent when some interface goes down\n{ \u0026#34;suppressed\u0026#34;: false, \u0026#34;devices\u0026#34;: [ { \u0026#34;system-ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34; } ], \u0026#34;eventname\u0026#34;: \u0026#34;interface-state-change\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;interface-state-change\u0026#34;, \u0026#34;rulename\u0026#34;: \u0026#34;interface-state-change\u0026#34;, \u0026#34;component\u0026#34;: \u0026#34;VPN\u0026#34;, \u0026#34;entry_time\u0026#34;: 1709277345253, \u0026#34;statcycletime\u0026#34;: 1709277345253, \u0026#34;message\u0026#34;: \u0026#34;The interface oper-state changed to down\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;Critical\u0026#34;, \u0026#34;severity_number\u0026#34;: 1, \u0026#34;uuid\u0026#34;: \u0026#34;9e2f7630-d504-4cdf-b808-fc8e29a6dd47\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;host-name\u0026#34;: \u0026#34;BR10\u0026#34;, \u0026#34;system-ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34;, \u0026#34;if-name\u0026#34;: \u0026#34;GigabitEthernet2\u0026#34;, \u0026#34;new-state\u0026#34;: \u0026#34;down\u0026#34;, \u0026#34;vpn-id\u0026#34;: \u0026#34;0\u0026#34; } ], \u0026#34;rule_name_display\u0026#34;: \u0026#34;Interface_State_Change\u0026#34;, \u0026#34;receive_time\u0026#34;: 1708843127894, \u0026#34;values_short_display\u0026#34;: [ { \u0026#34;host-name\u0026#34;: \u0026#34;BR10\u0026#34;, \u0026#34;system-ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34;, \u0026#34;if-name\u0026#34;: \u0026#34;GigabitEthernet2\u0026#34;, \u0026#34;new-state\u0026#34;: \u0026#34;down\u0026#34; } ], \u0026#34;system_ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34;, \u0026#34;host_name\u0026#34;: \u0026#34;BR10\u0026#34;, \u0026#34;acknowledged\u0026#34;: false, \u0026#34;active\u0026#34;: true } Let\u0026rsquo;s extract the most important information for us:\n\u0026quot;active\u0026quot;: true - Do we have a problem? Yes, issue is active or present \u0026quot;message\u0026quot;: \u0026quot;The interface oper...\u0026quot; - What is happening? \u0026quot;severity_number\u0026quot;: 1 - How serious it is? (we chose the number instead of the string on purpose) \u0026quot;uuid\u0026quot;: \u0026quot;9e2f7630-d504...d47\u0026quot; - Event identifier used by SD-WAN Manager \u0026quot;system_ip\u0026quot;: \u0026quot;1.1.10.1\u0026quot; - What device originated the event? \u0026quot;host_name\u0026quot;: \u0026quot;BR10\u0026quot; - More meaningful device identifier for humans Let\u0026rsquo;s see the notification when the interface comes up\n{ \u0026#34;suppressed\u0026#34;: false, \u0026#34;devices\u0026#34;: [ { \u0026#34;system-ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34; } ], \u0026#34;eventname\u0026#34;: \u0026#34;interface-state-change\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;interface-state-change\u0026#34;, \u0026#34;rulename\u0026#34;: \u0026#34;interface-state-change\u0026#34;, \u0026#34;component\u0026#34;: \u0026#34;VPN\u0026#34;, \u0026#34;entry_time\u0026#34;: 1709277482508, \u0026#34;statcycletime\u0026#34;: 1709277482508, \u0026#34;message\u0026#34;: \u0026#34;The interface oper-state changed to up\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;Medium\u0026#34;, \u0026#34;severity_number\u0026#34;: 3, \u0026#34;uuid\u0026#34;: \u0026#34;5486325c-d189-4467-9b5a-16acb1f28ec9\u0026#34;, \u0026#34;values\u0026#34;: [ { \u0026#34;host-name\u0026#34;: \u0026#34;BR10\u0026#34;, \u0026#34;system-ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34;, \u0026#34;if-name\u0026#34;: \u0026#34;GigabitEthernet2\u0026#34;, \u0026#34;new-state\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;vpn-id\u0026#34;: \u0026#34;0\u0026#34; } ], \u0026#34;rule_name_display\u0026#34;: \u0026#34;Interface_State_Change\u0026#34;, \u0026#34;receive_time\u0026#34;: 1708843265147, \u0026#34;values_short_display\u0026#34;: [ { \u0026#34;host-name\u0026#34;: \u0026#34;BR10\u0026#34;, \u0026#34;system-ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34;, \u0026#34;if-name\u0026#34;: \u0026#34;GigabitEthernet2\u0026#34;, \u0026#34;new-state\u0026#34;: \u0026#34;up\u0026#34; } ], \u0026#34;system_ip\u0026#34;: \u0026#34;1.1.10.1\u0026#34;, \u0026#34;host_name\u0026#34;: \u0026#34;BR10\u0026#34;, \u0026#34;acknowledged\u0026#34;: false, \u0026#34;cleared_events\u0026#34;: [ \u0026#34;9e2f7630-d504-4cdf-b808-fc8e29a6dd47\u0026#34; ], \u0026#34;active\u0026#34;: false } The two most important things are:\n\u0026quot;active\u0026quot;: false - Issue is no longer active or present \u0026quot;cleared_events\u0026quot;: [\u0026quot;9e2f7630-d504...d47]\u0026quot; - Event ID that is resolved One thing to know is that not all the events will behave in the same way. Some of them will not have a \u0026ldquo;cleared_events\u0026rdquo; entry so we would need to handle them differently if we want to automatically close them. Others might lack certain information depending on what we are monitoring.\nBefore coding any type of application, it\u0026rsquo;s important to know what you want to monitor and what you will be getting from the SD-WAN Manager so you can properly handle it.\nConfigure Webhooks on SD-WAN Manager On 20.12, it\u0026rsquo;s very simple to configure them, first enable the notification settings from Administration \u0026gt; Settings \u0026gt; Alarm Notifications Let\u0026rsquo;s define the rules to trigger our Webhooks. From Monitor \u0026gt; Logs \u0026gt; Alarm notification \u0026gt; Add Alarm Notification\nThings to know:\nYou can chose to monitor site or devices. Severity is crucial. To open incidents, you typically want to monitor critical and high severity incidents, to close them you need to monitor lower severities. The alarms you want to generate webhooks for, in our case BFD node down/up Omp node down/up Control node down/up Interface down/up Notice that I am only using HTTP in my webhook URL, it is recommended to use HTTPS for higher security. The 8080:/webhook comes from the app we build. Threshold will limit the amount of notifications sent per minute to this URL. 15 is enough for me, but probably not for a production environment Lastly, user and password in case your webhook app requires it. These values would be encoded and sent on the headers. Use dummy values if not required. Build the Webhook Server I have posted the code I used on the following GitHub Repo, here I will explain it in a simpler way.\nStep 1: Listen requests coming on port 8080 and destined to /webhook endpoint.\n// Listen upcoming requests http.HandleFunc(\u0026#34;/webhook\u0026#34;, handleWebhook) fmt.Println(\u0026#34;Server listening on port 8080...\u0026#34;) if err := http.ListenAndServe(\u0026#34;0.0.0.0:8080\u0026#34;, nil); err != nil { fmt.Printf(\u0026#34;Failed to start server: %v\u0026#34;, err) } Step 2. Check if the incoming request has an active status. If yes, create an incident on Service Now.\n//Verify if issue is active and create incident if data[\u0026#34;active\u0026#34;] == true { fmt.Println(\u0026#34;Opening Service Now incident...\u0026#34;) err := createIncident(data) Step 2.1 To open an incident with Service Now, we need to reformat the information that will be sent there.\nfunc createIncident(data map[string]interface{}) error { // Retrieve information to open incident issueId := data[\u0026#34;uuid\u0026#34;].(string) ruleName := data[\u0026#34;rule_name_display\u0026#34;].(string) title := data[\u0026#34;message\u0026#34;].(string) severity := data[\u0026#34;severity_number\u0026#34;].(float64) severityStr := strconv.FormatFloat(severity, \u0026#39;f\u0026#39;, -1, 64) device := \u0026#34;. Device \u0026#34; + data[\u0026#34;host_name\u0026#34;].(string) + \u0026#34;, System-ip \u0026#34; + data[\u0026#34;system_ip\u0026#34;].(string) // Construct JSON payload for creating incident in SNOW incidentData := map[string]interface{}{ \u0026#34;category\u0026#34;: \u0026#34;network\u0026#34;, \u0026#34;caller_id\u0026#34;: \u0026#34;vManage\u0026#34;, \u0026#34;short_description\u0026#34;: issueId, \u0026#34;description\u0026#34;: ruleName + \u0026#34; - \u0026#34; + title + device, \u0026#34;urgency\u0026#34;: severityStr, \u0026#34;impact\u0026#34;: severityStr, } ... Notice that we have stored the uuid coming from vmanage and used it as short_description for Service Now.\nStep 3.a If the status is not active, we check if there are any cleared_events included, this will give us high precision when closing incidents.\nif _, ok := data[\u0026#34;cleared_events\u0026#34;]; ok { ... clearedEvents, ok := data[\u0026#34;cleared_events\u0026#34;].([]interface{}) eventId := clearedEvents[0].(string) incidentExists, incident_id, err := getIncidentWithId(eventId) ... if incidentExists { err := closeIncident(incident_id) To close a case, we need to have the Service Now identifier called sys_id. To get it, we use the getIncidentWithId function.\nfunc getIncidentWithId(issueId string) (bool, string, error) { ... // Store Service Now \u0026#34;short_description\u0026#34; shortDescription, ok := incidentMap[\u0026#34;short_description\u0026#34;].(string) // Compare the short_description with the issueId if strings.Contains(shortDescription, issueId) { // If the short_description matches the issueId, return the incident id sys_id, ok := incidentMap[\u0026#34;sys_id\u0026#34;].(string) if !ok { continue } return true, sys_id, nil Step 3.b If the status is not active and there are no cleared events included, we are going to try and find the incident that was opened. There are three things we check:\nRule Name - The rule names for the events we are monitoring will have the following structure _node_. For example, BFD_Node_Down. If we are looking at BFD_Node_Up, we will change Up for Down and look for BFD_Node_Down on the incidents returned from Service Now. System Ip - Store the System-ip contained on the notification and match it on the description of each incident returned. Time - We store the opened_at time and check if it less than 12 hours. This is a totally subjective measure, but my idea is that that issues that take longer than 12 hours to resolve, would have to be verified by some human. func getIncidentWoutId(ruleName, sysIp string, openTime float64) (bool, string, error) { ... // IncidentMap holds the incidents from Service Now description := incidentMap[\u0026#34;description\u0026#34;].(string) snowTime := incidentMap[\u0026#34;opened_at\u0026#34;].(string) newRuleName := strings.Replace(ruleName, \u0026#34;Up\u0026#34;, \u0026#34;Down\u0026#34;, -1) // Compare Rule Name, system ip and time if strings.Contains(description, newRuleName) \u0026amp;\u0026amp; strings.Contains(description, sysIp) \u0026amp;\u0026amp; diffHours \u0026lt; 12 { sys_id, ok := incidentMap[\u0026#34;sys_id\u0026#34;].(string) sys_id, ok := incidentMap[\u0026#34;sys_id\u0026#34;].(string) if !ok { continue } return true, sys_id, nil Step 4. Close the case with the sys_id obtained through getIncidentWoutId function.\nif incidentExists { err := closeIncident(incident_id) if err != nil { fmt.Printf(\u0026#34;Error closing incident: %v\\n\u0026#34;, err) // Handle the error accordingly (e.g., log it, return, etc.) return } } else { fmt.Printf(\u0026#34;Incident doesn\u0026#39;t exist or is older than 12 hours\u0026#34;) } Demo We\u0026rsquo;ll start by running the Go app. Notice I am not using VS Code to run it(Cntrl + F5), but the terminal so we can allow incoming connections.\nInterface Notification Let\u0026rsquo;s shut down one of the service side interfaces on the router:\nBR10-1#config-transaction BR10-1(config)# interface GigabitEthernet 2 BR10-1(config-if)# shutdown BR10-1(config-if)# commit Commit complete. The server receives the notification and opens the incident. The incident number is the identifier in Service Now (sys_id)\nWe unshut the interface and the incident is closed\nNotice this notification has the \u0026ldquo;cleared_events\u0026rdquo; information, so its very easy to find that incident in Service Now. Also, the severity is medium, that\u0026rsquo;s why it\u0026rsquo;s important to set the right value on the Alarm Notification configuration.\nNotice that State is Resolved and Resolution Notes indicate the incident was automatically closed through Webhooks.\nControl Connections and BFD Notifications Let\u0026rsquo;s bring down CCs and BFD sessions by shutting down the transport interface\nBR10-1(config)# interface GigabitEthernet 1 BR10-1(config-if)# sh BR10-1(config-if)# commit Commit complete. Notifications were received and Incidents were created\nWhen we un-shut the interface we receive the notifications and also, we log the interface state status for interface Gig 1 and Tunnel1. This is all reflected in Service Now. These interface notifications were not delivered before because connectivity with SD-WAN Manager was lost.\nLessons Learned It\u0026rsquo;s very important to monitor the right severity level, otherwise we might miss notifications to properly close the incidents. SD-WAN Manager could be a bit chatty when generating alerts, the webhook threshold becomes very important and you should test to come with a number that works for your environment. Service Now will come up with a priority based on the Urgency and Impact used to create the ticket. Service Now could have some policies that will prevent you to close incidents if certain information is not present on the UI. Be a little bit familiar with UI policies and Data Policies. Although you can manually set the sys_id\u0026quot; on Service Now through APIs, I suggest to leave it alone as putting manual values could cause problems in the future and this field is supposed to be unique across your instance. Just use the auto-generated value. You can use public sites like this one, to easily see the content of the notifications while planning your use cases. Conclusion Webhooks are a great way to monitor our environment. Being notified exactly when an issue happens rather than relying in continuous polls, increase our ability to promptly log and react to whatever is going on. You can combine webhooks with other type of alerts, like emails or even chat (webex, slack, etc) in case you need to alert different teams. I hope this post has given you some ideas or trigger your curiosity.\nThanks for reading it!\n","permalink":"http://localhost:1313/tracking-sd-wan-incidents-with-service-now/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eAs networks evolve to provide better user experience and new technologies to manage the network are introduced, maintaining everything running smoothly has become increasingly difficult. One of the critical responsibilities of the operations team is tracking problems happening all over the network. Identifying them is just the beginning, then they need to be logged and driven to resolution. Multiply the amount of actions per incident and you have enough to keep your IT team busy all day long!\u003c/p\u003e","title":"Tracking SD-WAN incidents with Service Now"},{"content":"Introduction Software Defined Networks (SDNs) came with the promise of simplifying network management, enabling network teams to automate and adopt a programmatic approach. Cisco created solutions like - Catalyst Center, ACI, Meraki and Viptela SD-WAN. The latter introduced new controllers that changed the rules that dictate how the network operates.\nOrganization that were not ready to take the full SD-WAN leap, while still desiring the benefits of SDN principles had limited options, such as integrating with third-party systems or relying on Catalyst Center. While these solutions addressed some challenges, a significant gap remained unfilled until\u0026hellip; SD-Routing!\nWhat is SD-Routing? Simply put, SD-Routing is the middle ground between SDNs and SD-WAN. With SD-Routing, organizations could gradually adopt software-defined networking in their existing infrastructure. The idea behind it is to manage your non-sdwan (a.k.a \u0026ldquo;traditional\u0026rdquo;) network from a Single Pane of Glass called SD-WAN Manager (formerly vManage). It is possible to manage SD-WAN devices at the same time, of course!\nAs opposed to SD-WAN, there is no need to connect to the SD-WAN controller (formerly vSmart), thus the existing routing protocols remain in place and you get an additional security layer with the SD-WAN Validator (formely vBond) to allow the right devices to connect.\nBenefits Some of the benefits of SD-Routing include\nOnboarding - New devices could be easily and quickly onboarded using SD-WAN Manager. Configuration - Manage the configuration of your devices from a single place through a reusable method, called Configuration Groups. that will allow to scale quicker. Streamlined workflows for security policies and cloud connectivity. Monitoring - Monitor your devices, sites and applications. Get alerts and events, take advantage of SD-WAN\u0026rsquo;s Manager notification capabilities. Software management - Distribute, install and activate software images simply and quickly. Troubleshooting - Run different operations from vManage like SSH sessions, speed tests, traceroutes and more. Transition to SD-WAN- If you have plans to deploy SD-WAN, SD-Routing will be a great place to start getting familiar with the SD-WAN Manager and simplify the migration. SD-WAN Manager Overview Let me give you a quick tour of the SD-WAN Manager and a brief view of some of the features. If you are familiar with it, you are probably still going to be surprised by the new look on 20.13. Check it out!\nNetwork Overview When we first log-in, the network overview is presented so we can quickly determine how many devices and controllers are actively connected, application information, and more.\nHey, where did the SD-WAN Controller go?!\nSD-WAN Manager 20.13\nThe Magnetic look and feel will remind you of other Cisco Security Products or Meraki Dashboard, this is great to keep the experience consistent. Notice top right there is a SD-Routing toggle button, this is very useful to show the information concerning SD-Routing and the reason why we don\u0026rsquo;t see the SD-WAN Controller.\nMonitor Devices If we want to see more detailed device information we can visit the Devices tab. SD-WAN Manager is constantly refreshing this information so we have an accurate view. Notice devices are listed as SD-Routing.\nBR20 is not having a good health due to high memory utilization, could we find out when this started? Let\u0026rsquo;s double click on it.\nIt started going beyond 75% at around 12:30, this was because I activated performance monitor to get app performance information.\nLet\u0026rsquo;s focus on the left menu, look to all the available monitoring options including applications, security features and real time information. The troubleshooting section is the place to go to use the tools I listed earlier.\nConfiguration In 20.13/17.13 support for SD-Routing Configuration Groups was added. With them, you can build Feature Profiles based on parcels, which are single elements that together conform the entire router configuration. In 20.13 the following parcels are available.\nWe have the CLI configuration Profile to push whatever is not available through parcels. We can define variables to make our Profile reusable across multiple devices. You can also use an entire CLI Profile instead of using parcels.\nBy the way, if you have a mix of SD-WAN and SD-Routing devices, you will see both Configuration Groups listed there. SD-WAN has (for now) a richer set of parcels, this should be gradually shifted to support more non-CLI based configuration for SD-Routing.\nWorkflows The workflows library will help us easily achieve certain actions like onboard devices, security configuration, software upgrades and more.\nInstead of clicking around on multiple pages, we can navigate step by step as we are presented with everything we need. I personally like how workflows simplify things for us.\nCloud Connectivity Most likely your organization will use some kind of cloud connectivity, either to access apps or run workloads. SD-Routing has got you covered.\nYou can automate the connection to the cloud provider so your network will be extended to access those resources.\nThere are multiple options here. Some of this is also provided for SD-WAN so I recommend checking the config guides to see what is available for SD-Routing.\nConclusion The purpose of this post is to show what is possible with SD-Routing and the gap that it\u0026rsquo;s aiming to close. Adopting this technology could positively impact your operational workflows, enhance network agility, and optimize resource utilization. If your organization has not yet adopted any form of SDN, I invite you to think about your day-to-day processes and identify the main challenges, things that could be done more efficiently and how SD-Routing could help you with them.\nLet me know what you think and I will see you on the next one!\n","permalink":"http://localhost:1313/a-new-chapter-sd-routings-revolution-in-network-management/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eSoftware Defined Networks (SDNs) came with the promise of simplifying network management, enabling network teams to automate and adopt a programmatic approach. Cisco created solutions like - Catalyst Center, ACI, Meraki and \u003cem\u003eViptela SD-WAN\u003c/em\u003e. The latter introduced new controllers that changed the rules that dictate how the network operates.\u003c/p\u003e\n\u003cp\u003eOrganization that were not ready to take the full SD-WAN leap, while still desiring the benefits of SDN principles had limited options, such as integrating with third-party systems or relying on Catalyst Center. While these solutions addressed some challenges, a significant gap remained unfilled until\u0026hellip; SD-Routing!\u003c/p\u003e","title":"A New Chapter: SD-Routing's Revolution in Network Management"},{"content":"Introduction For the final post of this series, let\u0026rsquo;s explore the remaining option to handle traffic when SLA is not met: Fallback to best path. It was introduced on 20.5/17.5 and it provides more flexibility and enhanced path selection compared to the other options. Let\u0026rsquo;s understand why it was created.\nMotivation With the previous methods, traffic would either:\nBe dropped - Rarely used, specific use cases that apply to a small amount of environments. Be load balanced on the available paths - Widely used, however traffic could be using the worst performing path. Take the following example\nTunnel 2 is clearly having the worst performance, however with the load balance method, traffic could still use it based on the hashing algorithm. How do we overcome this situation? You probably guessed it: Fallback to best path\nFallback to Best Path Let\u0026rsquo;s see how the documentation describes it:\nWhen the data traffic does not meet any of the SLA class requirements, this feature allows you to select the best tunnel path criteria sequence using the Fallback Best Tunnel.\nCisco SD-WAN Manager uses best of worst (BOW) to find a best tunnel when no tunnel meets any of the SLA class requirements.\nhttps://www.cisco.com/c/en/us/td/docs/routers/sdwan/configuration/policies/ios-xe-17/policies-book-xe/application-aware-routing.html\nBest of Worst Let\u0026rsquo;s see how BOW works with the following example:\nThe SLA latency requirement is set to 8. None of the tunnels satisfy it, but Tunnel 1 is the closest one, making it the best of worse with a latency of 10.\nThe criteria(s) to choose the BOW is extremely flexible, in this example we used latency, other options could be:\nlatency - Only latency jitter - Only jitter loss - Only loss latency/jitter - First latency, if they are equal, then jitter latency/loss - First latency, if they are equal, then loss jitter/latency - First jitter, if they are equal, then latency . . . loss/jitter/latency - First loss, then jitter, then latency Variance Let\u0026rsquo;s go a step further, Tunnel 3 is also very close to the 8 ms latency, it wouldn\u0026rsquo;t be a bad idea to send traffic also on that tunnel. How do we achieve this? Well, we can implement a variance to accommodate small variations when choosing the best paths.\nContinuing with this example, let\u0026rsquo;s take a look at the BOW selection with a variance of 5 ms.\nBOW range = (best latency, best latency + variance) BOW range = (10, 15) The best latency across tunnels is 10 (Tunnel 1), notice this is not the latency configured on the SLA. With this variance, if any other tunnel has a latency between 10 - 15, it will also be chosen to send traffic. In our example, Tunnel 3 satisfies the condition, so now Tunnel 1 and Tunnel 3 will be used as Fallback Tunnels.\nAs you can see, Tunnel 2 is no longer considered. Great!\nConfiguration SLA Class To use this method, the first thing we need to do is modify our sla-class to indicate that it should look for the best performing path when SLA is not met. Our configuration looks like this:\nsla-class Custom-SLA loss 1 latency 250 jitter 100 fallback-best-tunnel criteria loss loss-variance 2 Notice we selected the Criteria to be Loss and a variance of 2. Variance is an optional parameter.\nAAR Policy Next, on the AAR policy we specify the action when SLA is not met: Fallback to Best path\nsequence 1 match source-ip 172.16.10.0/24 destination-ip 172.16.20.0/24 action sla-class Custom-SLA no sla-class strict sla-class preferred-color mpls sla-class fallback-to-best-path Let\u0026rsquo;s keep the same dynamic and build a diagram:\nScenarios Using the same topology let\u0026rsquo;s explore some situations\nMPLS compliant, Biz-internet/Private1 non-compliant I will initiate traffic from Branch10 -\u0026gt; Branch 20 and capture it with NWPI. MPLS has perfect KPI metrics (0, 0, 0)\nNotice how fallback to best path is set to no and traffic is matching the SLA and preferred color. Let\u0026rsquo;s also see the following verification command from BR10.\nBR10#show sdwan tunnel sla \u0026lt;. . .\u0026gt; tunnel sla-class 1 sla-name Custom-SLA sla-loss 1 sla-latency 250 sla-jitter 100 FALLBACK REMOTE T SLA SLA SRC DST SYSTEM LOCAL T REMOTE MEAN MEAN MEAN CLASS CLASS PROTO SRC IP DST IP PORT PORT IP COLOR COLOR LOSS LATENCY JITTER INDEX SLA CLASS NAME INDEX --------------------------------------------------------------------------------------------------------------------------------------------- gre 21.1.10.2 21.1.20.2 0 0 1.1.0.20 mpls mpls 0 0 0 0,1 __all_tunnels__, Custom-SLA None gre 21.1.10.2 31.1.20.2 0 0 1.1.0.20 mpls biz-internet 0 0 0 0,1 __all_tunnels__, Custom-SLA None gre 21.1.10.2 41.1.20.2 0 0 1.1.0.20 mpls private1 0 0 0 0,1 __all_tunnels__, Custom-SLA None Some comments about this output:\nThe fact that we see tunnels listed under Custom-SLA, tells us that there are tunnels meeting the loss, latency and jitter. This is expected as our MPLS is having perfect metrics. See that this sla-class has a numeric identifier of 1 - tunnel sla-class 1. You will see reference to this number shortly. Fallback SLA class index is set to none, this mean that these tunnels are not being used as fallback tunnels, this will become clear in a second. MPLS/Biz-inernet/Private1 non-compliant but meeting variance Now that we have no transports meeting the SLA, let\u0026rsquo;s check how NWPI will show it:\nNotice that now NWPI is indicating that Fallback to Best Path is in use.\nPrivate1 was chosen to send this particular flow, but are there any other tunnels that could be used? Let\u0026rsquo;s check BR10 again.\nBR10# show sdwan tunnel sla tunnel sla-class 0 sla-name __all_tunnels__ sla-loss 0 sla-latency 0 sla-jitter 0 FALLBACK REMOTE SLA SLA SRC DST SYSTEM T LOCAL T REMOTE MEAN MEAN MEAN CLASS CLASS PROTO SRC IP DST IP PORT PORT IP COLOR COLOR LOSS LATENCY JITTER INDEX SLA CLASS NAME INDEX ---------------------------------------------------------------------------------------------------------------------------------------- gre 21.1.10.2 21.1.20.2 0 0 1.1.0.20 mpls mpls 18 0 0 0 __all_tunnels__ None gre 21.1.10.2 31.1.20.2 0 0 1.1.0.20 mpls biz-internet 9 0 0 0 __all_tunnels__ None gre 21.1.10.2 41.1.20.2 0 0 1.1.0.20 mpls private1 11 0 0 0 __all_tunnels__ None gre 31.1.10.2 21.1.20.2 0 0 1.1.0.20 biz-internet mpls 4 0 0 0 __all_tunnels__ 1 gre 31.1.10.2 31.1.20.2 0 0 1.1.0.20 biz-internet biz-internet 2 0 0 0 __all_tunnels__ 1 gre 31.1.10.2 41.1.20.2 0 0 1.1.0.20 biz-internet private1 4 0 0 0 __all_tunnels__ 1 gre 41.1.10.2 21.1.20.2 0 0 1.1.0.20 private1 mpls 3 0 0 0 __all_tunnels__ 1 gre 41.1.10.2 31.1.20.2 0 0 1.1.0.20 private1 biz-internet 6 0 0 0 __all_tunnels__ None gre 41.1.10.2 41.1.20.2 0 0 1.1.0.20 private1 private1 3 0 0 0 __all_tunnels__ 1 tunnel sla-class 1 sla-name Custom-SLA sla-loss 1 sla-latency 250 sla-jitter 100 BR10# Comments about this output:\nThere are no tunnels meeting our Custom-SLA Even if MPLS is the preferred color, it is not considered because it doesn\u0026rsquo;t meet the loss variance range. There are 5 tunnels that satisfy the variance. Notice how some tunnels have Fallback SLA class index set to 1, meaning that they are serving as fallback for sla-class 1 (Custom-SLA). In this case the BOW is biz-internet - biz-internet tunnel with a mean loss of 2. The variance is set to 2, so BOW range is 2-4. Tunnels satisfying the BOW range, will be used to forward traffic as well. Depending on the load-balance hash, different tunnels will be chosen\nBR10#show sdwan policy service-path vpn 10 interface GigabitEthernet 3 source-ip 172.16.10.2 dest-ip 172.16.20.2 protocol 6 dest-port 22 Next Hop: GRE Source: 31.1.10.2 Destination: 21.1.20.2 Local Color: biz-internet Remote Color: mpls Remote System IP: 1.1.0.20 BR10#show sdwan policy service-path vpn 10 interface GigabitEthernet 3 source-ip 172.16.10.56 dest-ip 172.16.20.2 protocol 6 dest-port 24 Next Hop: GRE Source: 31.1.10.2 Destination: 41.1.20.2 Local Color: biz-internet Remote Color: private1 Remote System IP: 1.1.0.20 We can see two different tunnels biz-internet - mpls and biz-internet - private\nLatency out of compliance So far we have been playing only with loss, because the fallback criteria was set to loss. Let\u0026rsquo;s see what happens when a different criteria goes out of compliance. I will set the latency of the SLA to 15 ms. .\nBR10#show sdwan policy from-vsmart from-vsmart sla-class Custom-SLA loss 1 latency 15 jitter 100 fallback-best-tunnel criteria loss loss-variance 2 After introducing some latency, we see something interesting:\nBR10#show sdwan tunnel sla tunnel sla-class 0 sla-name __all_tunnels__ sla-loss 0 sla-latency 0 sla-jitter 0 FALLBACK REMOTE SLA SLA SRC DST SYSTEM T LOCAL T REMOTE MEAN MEAN MEAN CLASS CLASS PROTO SRC IP DST IP PORT PORT IP COLOR COLOR LOSS LATENCY JITTER INDEX SLA CLASS NAME INDEX ---------------------------------------------------------------------------------------------------------------------------------------- gre 21.1.10.2 21.1.20.2 0 0 1.1.0.20 mpls mpls 0 20 1 0 __all_tunnels__ 1 gre 21.1.10.2 31.1.20.2 0 0 1.1.0.20 mpls biz-internet 0 20 1 0 __all_tunnels__ 1 gre 21.1.10.2 41.1.20.2 0 0 1.1.0.20 mpls private1 0 20 0 0 __all_tunnels__ 1 gre 31.1.10.2 21.1.20.2 0 0 1.1.0.20 biz-internet mpls 0 21 2 0 __all_tunnels__ 1 gre 31.1.10.2 31.1.20.2 0 0 1.1.0.20 biz-internet biz-internet 0 21 2 0 __all_tunnels__ 1 gre 31.1.10.2 41.1.20.2 0 0 1.1.0.20 biz-internet private1 0 21 2 0 __all_tunnels__ 1 gre 41.1.10.2 21.1.20.2 0 0 1.1.0.20 private1 mpls 0 16 1 0 __all_tunnels__ 1 gre 41.1.10.2 31.1.20.2 0 0 1.1.0.20 private1 biz-internet 0 16 0 0 __all_tunnels__ 1 gre 41.1.10.2 41.1.20.2 0 0 1.1.0.20 private1 private1 0 16 0 0 __all_tunnels__ 1 tunnel sla-class 1 sla-name Custom-SLA sla-loss 1 sla-latency 15 sla-jitter 100 BR10# All of the tunnels are used as fallback tunnels because all of them have 0% loss! Is this the ideal situation? This is arguable, maybe for some types of traffic it\u0026rsquo;s fine, but for others you probably want to have a second or third criteria to pick the best fallback tunnels.\nMultiple BOW criteria For the last test, let\u0026rsquo;s see what happens when we select multiple criteria to select the BOW. I will add latency to the SLA criteria.\nBR10#show sdwan policy from-vsmart from-vsmart sla-class Custom-SLA loss 1 latency 15 jitter 100 fallback-best-tunnel criteria loss latency loss-variance 2 What we expect is that if any of the KPIs go out of compliance, the BOW will be decided based on:\nLowest mean loss. If there is a tie, then Lowest latency Let\u0026rsquo;s verify\nBR10#show sdwan tunnel sla tunnel sla-class 0 sla-name __all_tunnels__ sla-loss 0 sla-latency 0 sla-jitter 0 FALLBACK REMOTE SLA SLA SRC DST SYSTEM T LOCAL T REMOTE MEAN MEAN MEAN CLASS CLASS PROTO SRC IP DST IP PORT PORT IP COLOR COLOR LOSS LATENCY JITTER INDEX SLA CLASS NAME INDEX ---------------------------------------------------------------------------------------------------------------------------------------- gre 21.1.10.2 21.1.20.2 0 0 1.1.0.20 mpls mpls 0 20 1 0 __all_tunnels__ None gre 21.1.10.2 31.1.20.2 0 0 1.1.0.20 mpls biz-internet 0 20 1 0 __all_tunnels__ None gre 21.1.10.2 41.1.20.2 0 0 1.1.0.20 mpls private1 0 20 1 0 __all_tunnels__ None gre 31.1.10.2 21.1.20.2 0 0 1.1.0.20 biz-internet mpls 0 20 1 0 __all_tunnels__ None gre 31.1.10.2 31.1.20.2 0 0 1.1.0.20 biz-internet biz-internet 0 20 1 0 __all_tunnels__ None gre 31.1.10.2 41.1.20.2 0 0 1.1.0.20 biz-internet private1 0 21 1 0 __all_tunnels__ None gre 41.1.10.2 21.1.20.2 0 0 1.1.0.20 private1 mpls 0 16 0 0 __all_tunnels__ 1 gre 41.1.10.2 31.1.20.2 0 0 1.1.0.20 private1 biz-internet 0 16 0 0 __all_tunnels__ 1 gre 41.1.10.2 41.1.20.2 0 0 1.1.0.20 private1 private1 0 16 0 0 __all_tunnels__ 1 tunnel sla-class 1 sla-name Custom-SLA sla-loss 1 sla-latency 15 sla-jitter 100 BR10# It\u0026rsquo;s clear that the loss wasn\u0026rsquo;t used to come up with the BOW, otherwise we would see all the tunnels acting as fallback for SLA Class index 1. Instead, tunnels with lowest latency were selected. Notice that I could have added a latency variance to include other tunnels with similar numbers.\nConclusion Through the last three posts, we have witnessed AAR being a critical SD-WAN functionality to protect the SLA of our applications. I hope that after explaining and verifying different scenarios, you now have a better understanding and feel more confident to try AAR within your SD-WAN infrastructure.. The configuration guide is very complete, so get familiar with it and use it when you need it.\nLet me know your thoughts in the comments.\nSee you soon!\n","permalink":"http://localhost:1313/simplifying-aar-3-3-fallback-to-best-path/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eFor the final post of this series, let\u0026rsquo;s explore the remaining option to handle traffic when SLA is not met: \u003cem\u003eFallback to best path\u003c/em\u003e. It was introduced on 20.5/17.5 and it provides more flexibility and enhanced path selection compared to the other options. Let\u0026rsquo;s understand why it was created.\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eWith the \u003ca href=\"/demystifying-aar-understanding-different-scenarios/\"\u003eprevious methods\u003c/a\u003e, traffic would either:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBe dropped - Rarely used, specific use cases that apply to a small amount of environments.\u003c/li\u003e\n\u003cli\u003eBe load balanced on the available paths - Widely used, however traffic could be using the worst performing path.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTake the following example\u003c/p\u003e","title":"Simplifying AAR: 3/3 Fallback to best path"},{"content":"Introduction Think about routing technologies out there, most of them got really good at reacting to link failures and power outages with protocols like OSPF LFA/FRR, EIGRP feasible successor, BGP PIC, etc. However, they fall short when it comes to addressing issues like network performance degradation during brownouts caused by factors such as power fluctuations or link congestion. These scenarios introduce new challenges for which traditional routing protocols lack adequate tools. This is where Application Aware Routing comes to save the day.\nDespite causing some confusion among customers and SD-WAN learners, AAR represents a fundamental advantage of the technology. In this series, we will go through the basics, to understand key principles and behaviors that will enable us to study different scenarios. We will leverage NWPI to better understand how things are working, if you are not familiar with NWPI, I recommend you read this post I wrote about it.\nAAR in 5 lines Using IPSec tunnels formed between WAN Edges, BFD packets will be sent across them to measure the loss, latency and jitter (KPIs).\nUsers can define SLAs for different types of traffic (voice, web, video, etc.) and configure AAR policies to ensure traffic is sent through paths that satisfy the SLA.\nIf, at any point in time, the path is no longer satisfying the SLA, it will be automatically routed to a path that does.\nSimple, right?\nSLAs Service Level Agreement refers to the amount of loss, latency and jitter that an application can handle and still perform. The definition needs to be correct and realistic for the type of traffic and environment.\nExample, if we are defining the SLA for voice, we should know what are the acceptable loss, latency and jitter values. If our voice SLA looks something like this:\nLoss - 5% Latency - 350 ms Jitter - 200 ms It\u0026rsquo;s very likely that calls will not have good quality.\nHaving these values would be much better:\nLoss - 1% Latency - 150 ms Jitter - 50 ms Also, consider the nature of the environment when doing these definitions, there are places where providers could be less reliable, traffic could travel long distances, types of transport (satellite vs fiber), etc. You can use SD-WAM Manager\u0026rsquo;s historical KPIs data to build a baseline.\nSLA definition will look like this:\nsla-class Custom-SLA loss 1 latency 250 jitter 100 BFD Bidirectional Forwarding Detection protocol is used to quickly detect faults between two network devices. In SD-WAN, it is also used to measure loss, latency and jitter. The parameters we use to configure BFD will dictate how fast SD-WAN will detect and react to network problems.\nHello interval This interval represent how often a BFD echo packet will be sent. It is configured on a per color basis in milliseconds; the default is 1000 ms. This packet will go to the peer wan edge and back, this is how KPIs will be measured for each packet.\nsdwan interface GigabitEthernetX tunnel-interface color mpls hello-interval 1000 \u0026lt;\u0026lt;\u0026lt; Each router model has a defined tunnel scale (number of tunnels it can form). Changing the hello interval below 1s lowers the tunnel scale. Keep this in mind to avoid potential problems related to exceeding hardware capacity.\nApp route poll interval As the wan edge continues to send packets, the poll interval is the timer that will help organize the packets in buckets. These buckets will be used to calculate tunnel statistics. It\u0026rsquo;s configured on a per box basis (same for all colors).\nLet\u0026rsquo;s say we have a poll interval of 4000 ms. Every 4 seconds a new bucket is built, this bucket will have its own index. Default poll interval is 600,000 ms (10 minutes).\nTo configure the poll interval\nbfd app-route poll-interval 60000 The average loss, latency and jitter will be calculated for each poll interval. We can check this directly on SD-WAN Manager -\u0026gt; Monitor -\u0026gt; Real Time -\u0026gt; App Route Statistics\nApp route multiplier To configure the multiplier\nbfd app-route multiplier 3 The multiplier will indicate the amount of buckets that will be used to calculate the tunnel statistics that will make the traffic steer when network conditions worsen. For the illustration, a multiplier of 3 is used. By default the multiplier is set to 6.\nNotice each bucket contains 4 packets\nhello interval (s) x Poll interval (s)\nIn the real world, 4 packets would be too low. If we take all default values, we would end up with a bucket of 600 packets. Check the AAR Deployment Guide for more details and options.\nAfter filling up all the buckets, tunnel statistics will be calculated.\nWe can visualize the buckets and the mean calculations. Notice the values are the same for all the buckets. Don\u0026rsquo;t get confused with the avg loss, latency and jitter columns shown before.\nThis mechanism acts like a sliding window that will discard the oldest bucket to make room for the new one, recalculating tunnel statistics every poll interval.\nConfiguring AAR policy Now that we have a better understanding about SLAs and BFD, we can create the rules that will govern the behavior of our AAR policy. This is how it will look on SD-WAN Manager.\nThis is probably the simplest form we can configure. Google Apps traffic will be matched and transports matching our Custom-SLA will be used indiscriminately. In other words, if we have 3 different transports and all of them meet the SLA, traffic will be load balanced across them.\nWhat if we want traffic to prefer one of the transports? Well, we can specify a Preferred Color as below:\nIn this sequence, voice apps are matched, and transports\u0026rsquo; SLA should match Bussiness-Critical. MPLS will be preferred if it meets the SLA. The action when SLA is not met is set to Load Balance between available colors.\nAt a first glance, this looks very simple, but there are some details we need to know if we want to fully understand how traffic will behave under different circumstances.\nI invite you to read my next post where we will go through some scenarios to get a clear picture. See you there!\n","permalink":"http://localhost:1313/demystifying-aar-1-3-the-foundations/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThink about routing technologies out there, most of them got really good at reacting to link failures and power outages with protocols like OSPF LFA/FRR, EIGRP feasible successor, BGP PIC, etc. However, they fall short when it comes to addressing issues like network performance degradation during brownouts caused by factors such as power fluctuations or link congestion. These scenarios introduce new challenges for which traditional routing protocols lack adequate tools. This is where Application Aware Routing comes to save the day.\u003c/p\u003e","title":"Simplifying AAR:  1/3 The Foundations"},{"content":"Introduction Welcome back to the second installment of my series on Application Aware Routing (AAR). In my previous post, we discussed the essential concepts of Bidirectional Forwarding Detection (BFD) and Service Level Agreements (SLAs), laying the foundation for understanding how AAR optimizes network performance based on application requirements. We also briefly touched on tha AAR configuration with a simple example.\nNow, we continue by digging deeper on different AAR configurations, more specifically we will concentrate on Strict/Drop and Backup SLA Preferred Color behaviors.\nInitial Topology and Configuration Let\u0026rsquo;s start with a simple topology\nI am not limiting the tunnels between colors so we have a total of 4 tunnels on each wan edge.\nmpls - mpls mpls - biz-inet biz-inet- mpls biz-inet - biz-inet This is the BFD hello interval and app-route configuration for all the devices - this is the most relevant to AAR. If you are not familiar with it, I recommend you check my last post before continuing.\nhello-interval 1000 bfd app-route multiplier 3 bfd app-route poll-interval 120000 Scenario 1: SLA not Met - Strict/Drop Our AAR policy is testing against Business-Critical SLA. I will be playing with packet loss to demonstrate how traffic shifts.\nBR10#show sdwan policy from-vsmart from-vsmart sla-class Business-Critical loss 1 latency 250 jitter 100 from-vsmart app-route-policy _VPN10_AAR vpn-list VPN10 sequence 1 match source-ip 172.16.10.0/24 destination-ip 172.16.20.0/24 action sla-class Business-Critical sla-class strict sla-class preferred-color mpls sequence 11 match source-ip 172.16.20.0/24 destination-ip 172.16.10.0/24 action sla-class Business-Critical sla-class strict sla-class preferred-color mpls from-vsmart lists vpn-list VPN10 vpn 10 Let\u0026rsquo;s read what the documentation says about our config:\nsla-class preferred-color mpls\nsla-class sla-class-name preferred-color colorâ€”To set a specific tunnel to use when data traffic matches an SLA class, include the preferred-color option, specifying the color of the preferred tunnel. If more than one tunnel matches the SLA, traffic is sent to the preferred tunnel. If a tunnel of the preferred color is not available, traffic is sent through any tunnel that matches the SLA class. If no tunnel matches the SLA, data traffic is sent through any available tunnel\nhttps://www.cisco.com/c/en/us/td/docs/routers/sdwan/configuration/policies/ios-xe-17/policies-book-xe/application-aware-routing.html\nHere\u0026rsquo;s a diagram to better visualize it:\nNote than even if both biz-internet and MPLS match the SLA, only MPLS will be used.\nYes, Biz-internet will be used even if it\u0026rsquo;s not specified as a preferred color - from my experience, this is a frequent source of confusion as people expect that if mpls is not matching SLA, the SLA not met action will be executed without considering the rest of the colors.\nsla-class strict\nClick Strict/Drop to perform strict matching of the SLA class. If no data plane tunnel is available that satisfies the SLA criteria, traffic is dropped.\nhttps://www.cisco.com/c/en/us/td/docs/routers/sdwan/configuration/policies/ios-xe-17/policies-book-xe/application-aware-routing.html\nNow the diagram looks like this:\nTest 1 - MPLS/Biz-internet compliant I will initiate traffic from Branch10 -\u0026gt; Branch 20 and capture it with NWPI. Both MPLS and Biz-Internet are having perfect KPI metrics (0 loss, 0 latency, 0 jitter)\nBR10-PC1#ssh -l admin 172.16.20.2 Password: We can see the Actual Color is mpls and Tunnel Match Reason is Matched sla and pref encap color. Everything working according to our policy definition.\nTest 2 - Biz-Internet compliant, MPLS non-compliant Let\u0026rsquo;s introduce 3% of packet loss on the mpls link and check the results.\nNow, our mpls tunnel is above the 1% loss threshold, so it\u0026rsquo;s no longer eligible. We confirm the Biz-Internet tunnel is used because it matches the SLA - Tunnel match reason is matched sla and color any\nTest 3 - MPLS/Biz-Internet non-compliant Our last test for this scenario will be to mess the SLAs for both transports. Again, will introduce 3% packet loss on both mpls and biz-internet.\nAs expected, now the traffic is getting dropped on BR10 as there are no transports matching the SLA class. Note the DROP_REPORT indicates SdwanDataPolicyDrop\nScenario 2: Backup SLA Preferred Color Let\u0026rsquo;s explore a new scenario, we will add one transport for this one.\nOur policy configuration will have slight changes. One thing to remark is that when using Backup SLA preferred color option, the only available action when SLA is not met will be Load Balance.\nBR10#show sdwan policy from-vsmart from-vsmart sla-class Business-Critical loss 1 latency 250 jitter 100 from-vsmart app-route-policy _VPN10_AAR vpn-list VPN10 sequence 1 match source-ip 172.16.10.0/24 destination-ip 172.16.20.0/24 action backup-sla-preferred-color private1 sla-class Business-Critical no sla-class strict sla-class preferred-color mpls sequence 11 match source-ip 172.16.20.0/24 destination-ip 172.16.10.0/24 action backup-sla-preferred-color private1 sla-class Business-Critical no sla-class strict sla-class preferred-color mpls from-vsmart lists vpn-list VPN10 vpn 10 Again, let\u0026rsquo;s see what the documentation says:\nbackup-sla-preferred-color private1\nWhen no tunnel matches the SLA, you can choose how to handle the data traffic:\nbackup-sla-preferred-color colorsâ€”Direct the data traffic to a specific tunnel. Data traffic is sent out the configured tunnel if that tunnel interface is available; if that tunnel is unavailable, traffic is sent out another available tunnel. You can specify one or more colors.\nhttps://www.cisco.com/c/en/us/td/docs/routers/sdwan/configuration/policies/ios-xe-17/policies-book-xe/application-aware-routing.html\nTo put it visually\nTest 1 - MPLS/Private1 compliant, Biz-Internet non-compliant We will start with the assumption that biz-internet is not compliant with the SLA, so we have two transports available: MPLS and Private1. Let\u0026rsquo;s initiate our traffic.\nBR10-PC1#ssh -l admin 172.16.20.2 Password: Notice SLA strict is set to No and mpls is compliant and in use.\nTest 2 - Private1 compliant, MPLS/Biz-Internet non-compliant For the second test we are Introducing loss on the mpls transport. With this, now the only compliant transport is Private1, which also happens to be the Backup SLA preferred color. Let\u0026rsquo;s see how it looks.\nFrom the Tunnel Match Reason we can clearly see that the SLA is met through a color that is not the preferred one (private1). What do you think will happen if private1 becomes non-compliant?\nTest 3 - MPLS/Private1/Biz-Internet non-compliant Private1 is the only transport complying with the SLA, I will take care of that by introducing packet loss.\nAgain (!) private1 is used, BUT now traffic is matching the Default SLA, in other words, there are no tunnels matching the SLA so the tunnel marked as backup preferred, will be used.\nBonus Left - Private1 unavailable\nThe result after shutting down private1 interface, still without any tunnel matching the SLA. We can see a loose match is done on the tunnels (any available tunnel could be picked)\nRight - Biz-internet compliant, MPLS/Private1 non-compliant\nThe result after biz-internet is brought back to zero loss values with private1 not matching the SLA. Even if it\u0026rsquo;s not the preferred color, it still matches the SLA so it is selected.\nAs a final note, keep in mind AAR will always try to use tunnels that match the specified SLA, don\u0026rsquo;t get confused because the color names are not explicitly mentioned on the configuration.\nIn the next post, we will explore the remaining option Fallback to best path. Meet you there!\n","permalink":"http://localhost:1313/demystifying-aar-understanding-different-scenarios/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome back to the second installment of my series on Application Aware Routing (AAR). In my \u003ca href=\"/demystifying-aar-1-3-the-foundations/\"\u003eprevious post\u003c/a\u003e, we discussed the essential concepts of Bidirectional Forwarding Detection (BFD) and Service Level Agreements (SLAs), laying the foundation for understanding how AAR optimizes network performance based on application requirements. We also briefly touched on tha AAR configuration with a simple example.\u003c/p\u003e\n\u003cp\u003eNow, we continue by digging deeper on different AAR configurations, more specifically we will concentrate on \u003cem\u003eStrict/Drop\u003c/em\u003e and \u003cem\u003eBackup SLA Preferred Color\u003c/em\u003e behaviors.\u003c/p\u003e","title":"Simplifying AAR: 2/3 Understanding different scenarios"},{"content":"Motivation: The typical crisis scenario Imagine starting your day in the networking team, only to be bombarded with complaints about the main internal application being down. You start investigating. Where is the problem located? Is it isolated or affecting multiple places? When did the problem start? What is the impact?\nNext, you get someone to help you verify the basics. DHCP and DNS work. Gateway is reachable. Connectivity to other targets on the same DC is intermittent! It is starting to get weird\u0026hellip;\nAt this point, every minute counts and a proper troubleshooting process needs to be implemented to check all the devices involved and isolate the root of the problem. You start taking packet captures and traces on different points of the network, you check counters, bfd and ipsec sessions, look for inconsistencies on the omp/routing table, verify lengthy policy configurations and port parameters, one hop at a time. Two hours later (hopefully!) you and your team finally get to the root cause\u0026hellip;\nWhat Cisco did about it? Having in mind the level of complexity and the efforts needed to resolve network issues, Cisco created Network Wide Path Insights to make our WAN troubleshooting substantially easier. NWPI was first introduced in 20.4 and with every release it kept getting better. Fast forward to 20.9 and you have much better usability and insights that will help you quickly determine what is going on. Want to see it in action? Tag along!\nTopology We will use this scenario to run our NWPI trace. There is no centralized policy in place, as a result we have a full mesh and traffic could flow on any of the available tunnels.\nUnderstanding NWPI On vManage, navigate to the Tools section to find this feature. To start using it, the minimum information you need is:\nWhere traffic is generated (site ID) Segment of the network (vpn) Device is auto-populated. You can further refine your filters to capture exactly what you are looking for.\n20.12.2 NWPI\nOnce your filters are in place, start the trace and see the magic.\nInsight - basic view Let\u0026rsquo;s see the results of a small scp transfer from branch 10 to 20. This is the first piece of information we will see. Click the image to expand.\nFrom the above, we can determine:\nFlow direction - Traffic flowing from branch 10 to 20 WAN edges touching the traffic - BR10-1 and BR20-2 Flow information - Src/Dst IPs, ports, protocol, App, etc. Colors involved - MPLS -\u0026gt; Biz- Internet SD-WAN KPIs specific to the flow- Loss, latency and jitter Drop percentage - On the WAN and on devices Let\u0026rsquo;s click on the readout to learn more\nReadout New information is available to us:\nMatching entry on routing table - 172.10.20.0/24 coming from OMP and its respective metrics. Candidate and chosen path - Available paths shown and chosen one highlighted in green. Physical interfaces involved - Both for service and transport side Reason to choose this path - Routing, however keep in mind policies can override the routing table. If we stop here, we already have a lot of very useful information to understand the flow of the traffic, but what if we need to dig deeper? Well, let\u0026rsquo;s now explore the Advanced Views\nAdvanced Views If you know the Datapath Packet Trace, this information will look familiar. Essentially, it will tell us all the features that are executed by the device as the packet is being processed. Some examples could be ACLs, Policies, FW rules, DPI, Netflow and much more. There are instances where we need to determine if/why the device is dropping packets, this is the place to check!\nIn summary here is the additional information we can get:\nFeatures - Some of them will be configuration dependent, some will always be there on an SD-WAN environment. Drops - If a feature is dropping traffic, you have the information to know exactly why. Low level details around the features - Most of the time you won\u0026rsquo;t have to deal with this, but may be useful when you contact technical support. There is much more to this feature, but I think this is enough as an introduction. Before going to the conclusion, though, I would like to mention some of the use cases where this feature will come in handy.\nPoor application performance Policy validation Problem isolation DIA and SaaS validation/troubleshooting (yes, it can also give information of traffic destined to the internet) What other scenarios could you think about?\nConclusion NWPI is a great example of Cisco\u0026rsquo;s effort around serviceability - creating a tool that can help troubleshoot and resolve issues faster in a simple and efficient way. Check this guide to know more about it and be aware of the features introduced on the newest releases and more. In my experience, NWPI is not used enough mainly because it\u0026rsquo;s still unknown for a lot of customers and partners. I would encourage you to try it out and eventually incorporate it to your troubleshooting tool set, I am sure you will find some benefit to it.\n","permalink":"http://localhost:1313/network-wide-path-insights-an-introduction/","summary":"\u003ch3 id=\"motivation-the-typical-crisis-scenario\"\u003e\u003cstrong\u003eMotivation:\u003c/strong\u003e \u003cstrong\u003eThe typical crisis scenario\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eImagine starting your day in the networking team, only to be bombarded with complaints about the main internal application being down. You start investigating. Where is the problem located? Is it isolated or affecting multiple places? When did the problem start? What is the impact?\u003c/p\u003e\n\u003cp\u003eNext, you get someone to help you verify the basics. DHCP and DNS work. Gateway is reachable. Connectivity to other targets on the same DC is intermittent! It is starting to get weird\u0026hellip;\u003c/p\u003e","title":"What On Earth Is Happening On The Network? NWPI!"},{"content":"Describe characteristics and concepts of build /deploy tools such as Jenkins, Drone, or Travis CI In the last post we talked about the concepts of CI/CD, emphasizing the importance of the build phase as part of Continuous Integration process. It is now time to dig deeper on the following CI tools: Jenkins, Drone, Travis and GitLab.\nJenkins Jenkins is a free automation server (open source) designed to automate parts of the software development process, most notably the building, testing, and deployment of code changes. It is Java based, has a large community and has been out there for a while.\nOffering modeLanguage/StructureRootURLVM/Container(self-managed)GroovyJenkinsfilehttps://www.jenkins.io/\nIts pipeline is maintained in Groovy format and processed as Groovy script. This is the file Jenkins will look for by default\nHere is an example to understand the structure:\npipeline { agent any stages { stage(\u0026#39;Checkout\u0026#39;) { steps { \u0026lt;...\u0026gt; } } stage(\u0026#39;Build\u0026#39;) { steps { \u0026lt;...\u0026gt; } } stage(\u0026#39;Test\u0026#39;) { steps { \u0026lt;...\u0026gt; } } stage(\u0026#39;Deploy\u0026#39;) { steps { \u0026lt;...\u0026gt; } } } post{ \u0026lt;...\u0026gt; } } ``stages - 4 (checkout, build, test and deploy) steps - actions to execute on each of the stages post - Sections containing actions to execute based on the result of the pipeline (success, failure, always) Visit this page to learn more about the Pipeline\nJenkins also offers a User Interface for Pipeline visualization called Blue Ocean\nhttps://www.jenkins.io/blog/2016/05/26/introducing-blue-ocean/\nThe idea behind it is to improve the developer experience - make it easier to understand the Pipeline, make changes and monitor status and issues arise.\nDrone Built on a container technology. Feature set can be augmented through plug-ins that can work with source control technologies (GitHub, Bitbucket, GitLab, etc).\nTwo versions available - Free for Open Source and cloud service (not free).\nOffering modeLanguage/StructureRootURLSaaS/Container(self-managed)YAML.drone.ymlhttps://www.drone.io/\nIt is written in Go and uses the following .drone.yml structure:\nkind: pipeline name: example steps: - name: install image: node:14 commands: - npm install - name: test image: node:14 commands: - npm test - name: deploy image: alpine commands: - echo \u0026#34;Deploying the application\u0026#34; # Include deployment commands or scripts here kind - indicate this is a pipeline config (other values could be docker, kubernetes, secret) name - name of the pipeline steps - indicate individual sections of the pipeline. Each step has its own name, Docker image and commands to execute. Drones GUI will allow you\nCheck out this video by Harness to get familiar with Drone and the GUI.\nVisit Drone.io to learn more about this tool, plug-ins and to check the documentation.\nTravis CI Integrates very well with Github.com API. Offered as Software-as-a-Service and Open Source projects can be tested for free. VM/Container options are available as well.\nOffering modeLanguage/StructureRootURLSaaS/Container(self-managed)YAML.travis.ymlhttps://travis-ci.org\n--- language: node_js node_js: - \u0026#34;14\u0026#34; env: - CI_REG_IMG_APP=\u0026#34;my_node_app\u0026#34; before_script: - echo \u0026#34;Logging into DockerHub...\u0026#34; - echo \u0026#34;$DOCKER_PASSWORD\u0026#34; | docker login -u \u0026#34;$DOCKER_USERNAME\u0026#34; --password-stdin script: - echo \u0026#34;Building Docker image...\u0026#34; - docker build -t $CI_REG_IMG_APP . deploy: \u0026lt;...\u0026gt; language and node - Defines language to be used and versions env - Environment variable definition happens here before_script - Contains commands to be executed before running the main build script script - Contains the main build script deploy - Specifies deployment configuration I\u0026rsquo;d recommend checking out this video that shows how easy it is to integrate with Github and getting started with Travis.\nGitLab Available Enterprise and Community edition.\nGitlab has many features for DevOps:\nGit Repository Docker Image Registry Project Board CI/CD Pipelines Runners Offering modeLanguage/StructureRootURLSaaS/Container(self-managed)YAML.gitlab-ci.ymlhttps://gitlab.com\n--- stages: - build - test - deploy variables: NODE_VERSION: \u0026#34;14\u0026#34; before_script: - npm install -g yarn - yarn install build: stage: build script: - yarn build test: stage: test script: - yarn test deploy: stage: deploy script: - echo \u0026#34;Deploying application...\u0026#34; # Add deployment commands here only: - main stages - Defines stages of the pipeline variables - Defines global variables for the pipeline. before-script - Specifies commands to be executed before running any job in the pipeline build - Stage to build the application. It executes the yarn build command. test - Stage to run automated tests before deployment deploy - Responsible for deploying the application Here is an example of how this could be represented graphically.\nI would suggest exploring some of the tutorials to get familiar with GitLab.\n","permalink":"http://localhost:1313/ci-tools/","summary":"\u003ch2 id=\"describe-characteristics-and-concepts-of-build-deploy-tools-such-as-jenkins-drone-or-travis-ci\"\u003eDescribe characteristics and concepts of build /deploy tools such as Jenkins, Drone, or Travis CI\u003c/h2\u003e\n\u003cp\u003eIn the \u003ca href=\"/devops-ci-cd-pipeline/\"\u003elast post\u003c/a\u003e we talked about the concepts of CI/CD, emphasizing the importance of the \u003cstrong\u003e\u003cem\u003ebuild\u003c/em\u003e\u003c/strong\u003e phase as part of Continuous Integration process. It is now time to dig deeper on the following CI tools: Jenkins, Drone, Travis and GitLab.\u003c/p\u003e\n\u003ch3 id=\"jenkins\"\u003eJenkins\u003c/h3\u003e\n\u003cp\u003eJenkins is a \u003cstrong\u003e\u003cem\u003efree\u003c/em\u003e\u003c/strong\u003e automation server (open source) designed to automate parts of the software development process, most notably the building, testing, and deployment of code changes. It is Java based, has a large community and has been out there for a while.\u003c/p\u003e","title":"CI Tools"},{"content":"CI/CD concepts Continuous Integration/continuous delivery/deployment is a methodology to develop solutions. The idea is to automate as much as possible to move quickly and optimize the development process. The NetDevOps pipeline includes a building phase which we are going to focus on to better understand the concept of Continuous Integration.\nNetDevOps pipeline\nIn Continuous Integration (CI), a crucial idea is the frequent uploading of code into the code base, often multiple times a day. This involves the constant merging of developer work with the code base, and the early identification of issues through the use of testing.\nCI Diagram\nBenefits Reduce time to market Achieve revenue and results at a faster pace Smaller backlog Smaller code changes Fewer bugs Build Process Involves activities such as:\nIntegration: Merging code. If several people are working on a project, the changes of each of them, must be consolidated in a single place. Linting: Analyze code to identify programming errors, software defects, style and incorrect amounts of spaces. Some linting tools to know: Pylint Pyflakes Bandit Black Unit Testing: Test individual components of code, not the entire code base. Write tests first, then code - Test Driven Development (TDD). Some tools to know: Pytest Unittest Build: This stage implies that all linting and unit tests have been completed and code is ready to be deployed on staging environment. Here an \u0026ldquo;artifact\u0026rdquo; is built and stored independently. Suggestion: Explore the different tools shown above and know the basics of them\nCI tools These are some of the most common CI tools out there. In the next post we are going to explore them with more detail.\nCI ToolsURL**Travis CIhttps://travis-ci.orgJenkinshttps://jenkins.ioDronehttps://drone.ioGitLab CI**https://gitlab.com\nClick here to learn about them!\n","permalink":"http://localhost:1313/devops-ci-cd-pipeline/","summary":"\u003ch3 id=\"cicd-concepts\"\u003eCI/CD concepts\u003c/h3\u003e\n\u003cp\u003eContinuous Integration/continuous delivery/deployment is a methodology to develop solutions. The idea is to automate as much as possible to move quickly and optimize the development process. The NetDevOps pipeline includes a building phase which we are going to focus on to better understand the concept of Continuous Integration.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/wp-content/uploads/2024/01/Screenshot-2024-01-18-at-18.45.53.png\"\u003eNetDevOps pipeline\u003c/p\u003e\n\u003cp\u003eIn \u003cstrong\u003eContinuous Integration (CI)\u003c/strong\u003e, a crucial idea is the frequent uploading of code into the code base, often multiple times a day. This involves the constant merging of developer work with the code base, and the early identification of issues through the use of testing.\u003c/p\u003e","title":"1.0 CI/CD Pipeline"},{"content":" Hello! I'm Alex, and I love sharing my knowledge about networking and technology. I have been on the networking field for the past years. Currently contributing my expertise at Cisco, where I started at the front lines of R\u0026S TAC Team, after a couple of years, I transitioned to SD-WAN TAC Team. Currently, I'm a Customer Success Specialist for Enterprise technologies, particularly DNA Center and SD-WAN. I earned my CCIE Enterprise on 2022. I am also interested in automation, AI and technology in general. Thanks for being here. Hope you find my blog useful and entertaining. Let's connect! ðŸ‘‡ ","permalink":"http://localhost:1313/about/","summary":"\u003cdiv style=\"display: flex; align-items: center;\"\u003e\n    \u003cdiv style=\"flex: 1; padding-right: 20px;font-size: 17px;\"\u003e\n        \u003cdiv style=\"line-height: 1; margin-bottom: 1em; \"\u003e\u003c/div\u003e\n      \n\nHello! I'm Alex, and I love sharing my knowledge about networking and technology. I have been on the networking field for the past years.\n\u003cbr\u003e\u003cbr\u003e\nCurrently contributing my expertise at Cisco, where I started at the front lines of R\u0026S TAC Team, after a couple of years, I transitioned to SD-WAN TAC Team.\n\u003cbr\u003e\u003cbr\u003e\nCurrently, I'm a Customer Success Specialist for Enterprise technologies, particularly DNA Center and SD-WAN. I earned my CCIE Enterprise on 2022. I am also interested in automation, AI and technology in general. \n\u003cbr\u003e\u003cbr\u003e\nThanks for being here. Hope you find my blog useful and entertaining. Let's connect! ðŸ‘‡\n\u003cbr\u003e\u003cbr\u003e\n\u003cdiv style=\"display: flex; gap: 30px; align-items: center;\"\u003e\n\n  \u003ca href=\"https://www.linkedin.com/in/alexruizs/\" target=\"_blank\" style=\"margin-right: 1px;\"\u003e\n\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" style=\"width:30px; height:30px; fill: var(--primary);\"\u003e\u003cpath d=\"M22.23 0H1.77C.8 0 0 .77 0 1.72v20.56C0 23.23.8 24 1.77 24h20.46c.98 0 1.77-.77 1.77-1.72V1.72C24 .77 23.2 0 22.23 0zM7.27 20.1H3.65V9.24h3.62V20.1zM5.47 7.76h-.03c-1.22 0-2-.83-2-1.87 0-1.06.8-1.87 2.05-1.87 1.24 0 2 .8 2.02 1.87 0 1.04-.78 1.87-2.05 1.87zM20.34 20.1h-3.63v-5.8c0-1.45-.52-2.45-1.83-2.45-1 0-1.6.67-1.87 1.32-.1.23-.11.55-.11.88v6.05H9.28s.05-9.82 0-10.84h3.63v1.54a3.6 3.6 0 0 1 3.26-1.8c2.39 0 4.18 1.56 4.18 4.89v6.21z\"/\u003e\u003c/svg\u003e\n  \u003c/a\u003e\n\n  \u003c!-- GitHub Icon --\u003e\n  \u003ca href=\"https://github.com/aruiz-p\" target=\"_blank\" style=\"margin-right: 1px;\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" style=\"width: 30px; height: 30px; fill: var(--primary);\"\u003e\u003cpath d=\"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577v-2.234c-3.338.724-4.033-1.415-4.033-1.415-.546-1.385-1.333-1.754-1.333-1.754-1.089-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.775.418-1.305.762-1.605-2.665-.3-5.466-1.333-5.466-5.93 0-1.31.465-2.381 1.235-3.221-.123-.303-.535-1.523.117-3.176 0 0 1.008-.322 3.3 1.23.957-.266 1.983-.398 3.003-.404 1.02.006 2.047.138 3.006.404 2.29-1.552 3.296-1.23 3.296-1.23.653 1.653.241 2.873.118 3.176.77.84 1.231 1.911 1.231 3.221 0 4.61-2.805 5.625-5.475 5.92.429.372.824 1.104.824 2.222v3.293c0 .322.218.694.825.576C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\"/\u003e\u003c/svg\u003e  \n  \u003c/a\u003e\n\n  \u003c!-- Gmail Icon --\u003e\n\u003ca href=\"mailto:netwithalex@gmail.com\" target=\"_blank\" style=\"margin-right: 1px;\"\u003e\n  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" style=\"width: 30px; height: 30px;\"\u003e\n    \u003c!-- Background --\u003e\n    \u003crect width=\"24\" height=\"24\" fill=\"var(--theme)\" /\u003e\n    \u003c!-- Envelope Outline --\u003e\n    \u003cpath d=\"M12 12.713L.015 5.328V19.2A2.8 2.8 0 002.8 22h18.4a2.8 2.8 0 002.8-2.8V5.328L12 12.713zm11.985-7.385v-.2a2.8 2.8 0 00-2.8-2.8H2.8A2.8 2.8 0 000 5.328l12 7.679 12-7.679z\" fill=\"none\" stroke=\"var(--primary)\" stroke-width=\"2.5\" /\u003e\n  \u003c/svg\u003e\n\u003c/a\u003e\n\u003c/div\u003e\n\n\n    \u003c/div\u003e\n    \u003cdiv style=\"flex: 1;\"\u003e\n      \u003cimg src=\"/wp-content/uploads/2024/01/IMG_8522-e1704915321848.jpeg\" alt=\"Example Image\" style=\"max-width: 100%; height: auto;border: 5px solid transparent;border-radius: 20px;\"\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e","title":"About me."},{"content":"20% 1.0 CI/CD Pipeline\n1.1 Describe characteristics and concepts of build /deploy tools such as Jenkins, Drone, or Travis CI 1.2 Identify the sequence, components, and integrations to implement a CI/CD pipeline for a given scenario 1.3 Troubleshoot issues with a CI/CD pipeline such as code-based failures, pipeline issues, and tool incompatibility 1.4 Identify tests to integrate into a CI/CD pipeline for a given scenario 1.5 Identify release deployment strategy (canary, rollbacks, and blue/green) for a given scenario 1.6 Diagnose code dependency management issues including API, tool chain, and libraries 15% 2.0 Packaging and Delivery of Applications\n2.1 Identify the steps to containerize an application 2.2 Identify steps to deploy multiple microservice applications 2.3 Evaluate microservices and container architecture diagrams based on technical and business requirements (security, performance, stability, and cost) 2.4 Identify safe handling practices for configuration items, application parameters, and secrets 2.5 Construct a Docker file to address application specifications 2.6 Describe the usage of golden images to deploy applications 20% 3.0 Automating Infrastructure\n3.1 Describe how to integrate DevOps practices into an existing organization structure 3.2 Describe the use of configuration management tools to automate infrastructure services such as Ansible, Puppet, Terraform, and Chef 3.3 Construct an Ansible playbook to automate an application deployment of infrastructure services 3.4 Construct a Terraform configuration to automate an application deployment of infrastructure services 3.5 Describe the practice and benefits of Infrastructure as Code 3.6 Design a pre-check validation of the network state in a CI/CD pipeline for a given scenario 3.7 Design a pre-check validation of the application infrastructure in a CI/CD pipeline for a given scenario 3.8 Describe the concepts of extending DevOps practices to the network for NetDevOps 3.9 Identify the requirements such as memory, disk I/O, network, and CPU needed to scale the application or service 15% 4.0 Cloud and Multicloud\n4.1 Describe the concepts and objects of Kubernetes 4.2 Deploy applications to a Kubernetes cluster 4.3 Utilize objects of Kubernetes to build a deployment to meet requirements 4.4 Interpret the pipeline for continuous delivery of a Drone configuration file 4.5 Validate the success of an application deployment on Kubernetes 4.6 Describe method and considerations to deploy an application to multiple environments such as multiple cloud providers, high availability configurations, disaster recovery configurations, and testing cloud portability 4.7 Describe the process of tracking and projecting costs when consuming public cloud 4.8 Describe benefits of infrastructure as code for repeatable public cloud consumption 4.9 Compare cloud services strategies (build versus buy) 20% 5.0 Logging, Monitoring, and Metrics\n5.1 Identify the elements of log and metric systems to facilitate application troubleshooting such as performance issues and streaming telemetry logs 5.2 Implement a log collection and reporting system for applications 5.2.a aggregate logs from multiple related applications 5.2.b search capabilities 5.2.c reporting capabilities 5.3 Troubleshoot a distributed application using AppDyanmics with Application Performance Monitoring 5.4 Describe the principles of chaos engineering 5.5 Construct Python scripts that use APIs to accomplish these tasks 5.5.a build a monitoring dashboard 5.5.b notify Webex Teams space 5.5.c responding to alerts and outages 5.5.d creating notifications 5.5.e health check monitoring 5.5.f opening and closing incidents 5.6 Identify additional application requirements to provide visibility into application health and performance 5.7 Describe Kubernetes capabilities related to logging, monitoring, and metrics 5.8 Describe the integration of logging, monitoring and alerting in a CI/CD pipeline design 10% 6.0 Security\n6.1 Identify methods to secure an application and infrastructure during production and testing in a CI/CD pipeline 6.2 Identify methods to implement a secure software development life cycle ","permalink":"http://localhost:1313/study-devops-blueprint/","summary":"\u003cp\u003e\u003cstrong\u003e\u003ca href=\"/devops-ci-cd-pipeline/\"\u003e20% 1.0 CI/CD Pipeline\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/ci-tools/\"\u003e1.1 Describe characteristics and concepts of build /deploy tools such as Jenkins, Drone, or Travis CI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e1.2 Identify the sequence, components, and integrations to implement a CI/CD pipeline for a given scenario\u003c/li\u003e\n\u003cli\u003e1.3 Troubleshoot issues with a CI/CD pipeline such as code-based failures, pipeline issues, and tool incompatibility\u003c/li\u003e\n\u003cli\u003e1.4 Identify tests to integrate into a CI/CD pipeline for a given scenario\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/ci-cd-deployment-strategies/\"\u003e1.5 Identify release deployment strategy (canary, rollbacks, and blue/green) for a given scenario\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e1.6 Diagnose code dependency management issues including API, tool chain, and libraries\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e15% 2.0 Packaging and Delivery of Applications\u003c/strong\u003e\u003c/p\u003e","title":"DEVNET DevOps (300-910) - Blueprint"}]